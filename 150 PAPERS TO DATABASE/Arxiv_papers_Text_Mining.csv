"","titles","authors","subjects","abstracts","metadata"
"1","dissecting deep rl with high update ratios: combatting value overestimation and divergence","marcel hussing, claas voelcker, igor gilitschenski, amir-massoud farahmand, eric eaton","machine learning (cs.lg)","we show that deep reinforcement learning can maintain its ability to learn without resetting network parameters in settings where the number of gradient updates greatly exceeds the number of environment samples. under such large update-to-data ratios, a recent study by nikishin et al. (2022) suggested the emergence of a primacy bias, in which agents overfit early interactions and downplay later experience, impairing their ability to learn. in this work, we dissect the phenomena underlying the primacy bias. we inspect the early stages of training that ought to cause the failure to learn and find that a fundamental challenge is a long-standing acquaintance: value overestimation. overinflated q-values are found not only on out-of-distribution but also in-distribution data and can be traced to unseen action prediction propelled by optimizer momentum. we employ a simple unit-ball normalization that enables learning under large update ratios, show its efficacy on the widely used dm_control suite, and obtain strong performance on the challenging dog tasks, competitive with model-based approaches. our results question, in parts, the prior explanation for sub-optimal learning due to overfitting on early data.","sat, 9 mar 2024 19:56:40 utc (6,450 kb)"
"2","a tunable reflection surface with independently variable phase and slope","omran abbas, anas chaaban, loic markley","signal processing (eess.sp)","a reconfigurable intelligent surface (ris) is an essential component in the architecture of the next generation of wireless communication systems. an ris is deployed to provide a controllability to the multi-path environment between the transmitter and the receiver, which becomes critical when the line-of-sight signal between them is blocked. in this work, we design an electrically tunable linearly polarized ris at 2.5 ghz that yields a controllable reflection phase and phase-frequency slope; in other words, we add tunability of the phase-frequency slope to the tunability of the resonance center frequency. the proposed design consists of two layers of unit cells placed over a ground plane, with dog-bone-shaped elements in the top layer and patch elements in the bottom layer. each patch and dog-bone element is loaded with a varactor, whose reverse bias voltage is controlled to provide a phase-frequency profile with a slope value of 9 degrees/mhz or 0.95 degrees/mhz, and a phase shift range of 320 degrees.","mon, 4 mar 2024 22:36:12 utc (24 kb)"
"3","overestimation, overfitting, and plasticity in actor-critic: the bitter lesson of reinforcement learning","michal nauman, michał bortkiewicz, mateusz ostaszewski, piotr miłoś, tomasz trzciński, marek cygan","machine learning (cs.lg)","recent advancements in off-policy reinforcement learning (rl) have significantly improved sample efficiency, primarily due to the incorporation of various forms of regularization that enable more gradient update steps than traditional agents. however, many of these techniques have been tested in limited settings, often on tasks from single simulation benchmarks and against well-known algorithms rather than a range of regularization approaches. this limits our understanding of the specific mechanisms driving rl improvements. to address this, we implemented over 60 different off-policy agents, each integrating established regularization techniques from recent state-of-the-art algorithms. we tested these agents across 14 diverse tasks from 2 simulation benchmarks. our findings reveal that while the effectiveness of a specific regularization setup varies with the task, certain combinations consistently demonstrate robust and superior performance. notably, a simple soft actor-critic agent, appropriately regularized, reliably solves dog tasks, which were previously solved mainly through model-based approaches.","fri, 1 mar 2024 13:25:10 utc (29,011 kb)"
"4","finer: investigating and enhancing fine-grained visual concept recognition in large vision language models","jeonghwan kim, heng ji","computer vision and pattern recognition (cs.cv)","recent advances in instruction-tuned large vision-language models (lvlms) have imbued the models with the ability to generate high-level, image-grounded explanations with ease. while such capability is largely attributed to the rich world knowledge contained within the large language models (llms), our work reveals their shortcomings in fine-grained visual categorization (fgvc) across six different benchmark settings. most recent state-of-the-art lvlms like llava-1.5, instructblip and gpt-4v not only severely deteriorate in terms of classification performance, e.g., average drop of 65.58 in em for stanford dogs for llava-1.5, but also struggle to generate an accurate explanation with detailed attributes based on the concept that appears within an input image despite their capability to generate holistic image-level descriptions. in-depth analyses show that instruction-tuned lvlms exhibit modality gap, showing discrepancy when given textual and visual inputs that correspond to the same concept, preventing the image modality from leveraging the rich parametric knowledge within the llms. in an effort to further the community's endeavor in this direction, we propose a multiple granularity attribute-centric evaluation benchmark, finer, which aims to establish a ground to evaluate lvlms' fine-grained visual comprehension ability and provide significantly improved explainability.","mon, 26 feb 2024 05:43:51 utc (28,563 kb)"
"5","phonetic and lexical discovery of a canine language using hubert","xingyuan li, sinong wang, zeyu xie, mengyue wu, kenny q. zhu","sound (cs.sd)","this paper delves into the pioneering exploration of potential communication patterns within dog vocalizations and transcends traditional linguistic analysis barriers, which heavily relies on human priori knowledge on limited datasets to find sound units in dog vocalization. we present a self-supervised approach with hubert, enabling the accurate classification of phoneme labels and the identification of vocal patterns that suggest a rudimentary vocabulary within dog vocalizations. our findings indicate a significant acoustic consistency in these identified canine vocabulary, covering the entirety of observed dog vocalization sequences. we further develop a web-based dog vocalization labeling system. this system can highlight phoneme n-grams, present in the vocabulary, in the dog audio uploaded by users.","sun, 25 feb 2024 04:35:45 utc (2,348 kb)"
"6","barking dogs: a fréchet distance variant for detour detection","ivor van der hoog, fabian klute, irene parada, patrick schnider","computational geometry (cs.cg)","imagine you are a dog behind a fence $q$ and a hiker is passing by at constant speed along the hiking path $p$. in order to fulfil your duties as a watchdog, you desire to bark as long as possible at the human. however, your barks can only be heard in a fixed radius $\rho$ and, as a dog, you have bounded speed $s$. can you optimize your route along the fence $q$ in order to maximize the barking time with radius $\rho$, assuming you can run backwards and forward at speed at most $s$? we define the barking distance from a polyline $p$ on $n$ vertices to a polyline $q$ on $m$ vertices as the time that the hiker stays in your barking radius if you run optimally along $q$. this asymmetric similarity measure between two curves can be used to detect outliers in $q$ compared to $p$ that other established measures like the fréchet distance and dynamic time warping fail to capture at times. we consider this measure in three different settings. in the discrete setting, the traversals of $p$ and $q$ are both discrete. for this case we show that the barking distance from $p$ to $q$ can be computed in $o(nm\log s)$ time. in the semi-discrete setting, the traversal of $q$ is continuous while the one of $p$ is again discrete. here, we show how to compute the barking distance in time $o(nm\log (nm))$. finally, in the continuous setting in which both traversals are continuous, we show that the problem can be solved in polynomial time. for all the settings we show that, assuming seth, no truly subquadratic algorithm can exist.","tue, 20 feb 2024 17:22:11 utc (528 kb)"
"7","see spot guide: accessible interfaces for an assistive quadruped robot","rayna hata, narit trikasemsak, andrea giudice, stacy a. doore","robotics (cs.ro)","while there is no replacement for the learned expertise, devotion, and social benefits of a guide dog, there are cases in which a robot navigation assistant could be helpful for individuals with blindness or low vision (blv). this study investigated the potential for an industrial agile robot to perform guided navigation tasks. we developed two interface prototypes that allowed for spatial information between a human-robot pair: a voice-based app and a flexible, responsive handle. the participants (n=21) completed simple navigation tasks and a post-study survey about the prototype functionality and their trust in the robot. all participants successfully completed the navigation tasks and demonstrated the interface prototypes were able to pass spatial information between the human and the robot. future work will include expanding the voice-based app to allow the robot to communicate obstacles to the handler and adding haptic signals to the handle design.","fri, 16 feb 2024 23:17:54 utc (6,976 kb)[v2] tue, 20 feb 2024 01:35:12 utc (6,976 kb)"
"8","point and instruct: enabling precise image editing by unifying direct manipulation and text instructions","alec helbling, seongmin lee, polo chau","artificial intelligence (cs.ai)","machine learning has enabled the development of powerful systems capable of editing images from natural language instructions. however, in many common scenarios it is difficult for users to specify precise image transformations with text alone. for example, in an image with several dogs, it is difficult to select a particular dog and move it to a precise location. doing this with text alone would require a complex prompt that disambiguates the target dog and describes the destination. however, direct manipulation is well suited to visual tasks like selecting objects and specifying locations. we introduce point and instruct, a system for seamlessly combining familiar direct manipulation and textual instructions to enable precise image manipulation. with our system, a user can visually mark objects and locations, and reference them in textual instructions. this allows users to benefit from both the visual descriptiveness of natural language and the spatial precision of direct manipulation.","mon, 5 feb 2024 16:23:07 utc (5,289 kb)"
"9","tuning-free stochastic optimization","ahmed khaled, chi jin","optimization and control (math.oc)","large-scale machine learning problems make the cost of hyperparameter tuning ever more prohibitive. this creates a need for algorithms that can tune themselves on-the-fly. we formalize the notion of ""tuning-free"" algorithms that can match the performance of optimally-tuned optimization algorithms up to polylogarithmic factors given only loose hints on the relevant problem parameters. we consider in particular algorithms that can match optimally-tuned stochastic gradient descent (sgd). when the domain of optimization is bounded, we show tuning-free matching of sgd is possible and achieved by several existing algorithms. we prove that for the task of minimizing a convex and smooth or lipschitz function over an unbounded domain, tuning-free optimization is impossible. we discuss conditions under which tuning-free optimization is possible even over unbounded domains. in particular, we show that the recently proposed dog and dowg algorithms are tuning-free when the noise distribution is sufficiently well-behaved. for the task of finding a stationary point of a smooth and potentially nonconvex function, we give a variant of sgd that matches the best-known high-probability convergence rate for tuned sgd at only an additional polylogarithmic cost. however, we also give an impossibility result that shows no algorithm can hope to match the optimal expected convergence rate for tuned sgd with high probability.","mon, 12 feb 2024 16:59:06 utc (66 kb)"
"10","complete instances mining for weakly supervised instance segmentation","zecheng li, zening zeng, yuqi liang, jin-gang yu","computer vision and pattern recognition (cs.cv)","weakly supervised instance segmentation (wsis) using only image-level labels is a challenging task due to the difficulty of aligning coarse annotations with the finer task. however, with the advancement of deep neural networks (dnns), wsis has garnered significant attention. following a proposal-based paradigm, we encounter a redundant segmentation problem resulting from a single instance being represented by multiple proposals. for example, we feed a picture of a dog and proposals into the network and expect to output only one proposal containing a dog, but the network outputs multiple proposals. to address this problem, we propose a novel approach for wsis that focuses on the online refinement of complete instances through the use of maskiou heads to predict the integrity scores of proposals and a complete instances mining (cim) strategy to explicitly model the redundant segmentation problem and generate refined pseudo labels. our approach allows the network to become aware of multiple instances and complete instances, and we further improve its robustness through the incorporation of an anti-noise strategy. empirical evaluations on the pascal voc 2012 and ms coco datasets demonstrate that our method achieves state-of-the-art performance with a notable margin. our implementation will be made available at this https url.","mon, 12 feb 2024 13:16:47 utc (10,083 kb)"
"11","towards robotic companions: understanding handler-guide dog interactions for informed guide dog robot design","hochul hwang, hee-tae jung, nicholas a giudice, joydeep biswas, sunghoon ivan lee, donghyun kim","robotics (cs.ro)","dog guides are favored by blind and low-vision (blv) individuals for their ability to enhance independence and confidence by reducing safety concerns and increasing navigation efficiency compared to traditional mobility aids. however, only a relatively small proportion of blv individuals work with dog guides due to their limited availability and associated maintenance responsibilities. there is considerable recent interest in addressing this challenge by developing legged guide dog robots. this study was designed to determine critical aspects of the handler-guide dog interaction and better understand handler needs to inform guide dog robot development. we conducted semi-structured interviews and observation sessions with 23 dog guide handlers and 5 trainers. thematic analysis revealed critical limitations in guide dog work, desired personalization in handler-guide dog interaction, and important perspectives on future guide dog robots. grounded on these findings, we discuss pivotal design insights for guide dog robots aimed for adoption within the blv community.","fri, 9 feb 2024 21:24:13 utc (44,908 kb)"
"12","gazegpt: augmenting human capabilities using gaze-contingent contextual ai for smart eyewear","robert konrad, nitish padmanaban, j. gabriel buckmaster, kevin c. boyle, gordon wetzstein","human-computer interaction (cs.hc)","multimodal large language models (lmms) excel in world knowledge and problem-solving abilities. through the use of a world-facing camera and contextual ai, emerging smart accessories aim to provide a seamless interface between humans and lmms. yet, these wearable computing systems lack an understanding of the user's attention. we introduce gazegpt as a new user interaction paradigm for contextual ai. gazegpt uses eye tracking to help the lmm understand which object in the world-facing camera view a user is paying attention to. using extensive user evaluations, we show that this gaze-contingent mechanism is a faster and more accurate pointing mechanism than alternatives; that it augments human capabilities by significantly improving their accuracy in a dog-breed classification task; and that it is consistently ranked as more natural than head- or body-driven selection mechanisms for contextual ai. moreover, we prototype a variety of application scenarios that suggest gazegpt could be of significant value to users as part of future ai-driven personal assistants.","tue, 30 jan 2024 18:02:44 utc (2,702 kb)[v2] wed, 31 jan 2024 05:21:13 utc (2,702 kb)"
"13","unveiling the unseen: identifiable clusters in trained depthwise convolutional kernels","zahra babaiee, peyman m. kiasari, daniela rus, radu grosu","machine learning (cs.lg)","recent advances in depthwise-separable convolutional neural networks (ds-cnns) have led to novel architectures, that surpass the performance of classical cnns, by a considerable scalability and accuracy margin. this paper reveals another striking property of ds-cnn architectures: discernible and explainable patterns emerge in their trained depthwise convolutional kernels in all layers. through an extensive analysis of millions of trained filters, with different sizes and from various models, we employed unsupervised clustering with autoencoders, to categorize these filters. astonishingly, the patterns converged into a few main clusters, each resembling the difference of gaussian (dog) functions, and their first and second-order derivatives. notably, we were able to classify over 95\% and 90\% of the filters from state-of-the-art convnextv2 and convnext models, respectively. this finding is not merely a technological curiosity; it echoes the foundational models neuroscientists have long proposed for the vision systems of mammals. our results thus deepen our understanding of the emergent properties of trained ds-cnns and provide a bridge between artificial and biological visual processing systems. more broadly, they pave the way for more interpretable and biologically-inspired neural network designs in the future.","thu, 25 jan 2024 19:05:53 utc (16,561 kb)"
"14","dna sequence classification with compressors","şükrü ozan","genomics (q-bio.gn)","recent studies in dna sequence classification have leveraged sophisticated machine learning techniques, achieving notable accuracy in categorizing complex genomic data. among these, methods such as k-mer counting have proven effective in distinguishing sequences from varied species like chimpanzees, dogs, and humans, becoming a staple in contemporary genomic research. however, these approaches often demand extensive computational resources, posing a challenge in terms of scalability and efficiency. addressing this issue, our study introduces a novel adaptation of jiang et al.'s compressor-based, parameter-free classification method, specifically tailored for dna sequence analysis. this innovative approach utilizes a variety of compression algorithms, such as gzip, brotli, and lzma, to efficiently process and classify genomic sequences. not only does this method align with the current state-of-the-art in terms of accuracy, but it also offers a more resource-efficient alternative to traditional machine learning methods. our comprehensive evaluation demonstrates the proposed method's effectiveness in accurately classifying dna sequences from multiple species. we present a detailed analysis of the performance of each algorithm used, highlighting the strengths and limitations of our approach in various genomic contexts. furthermore, we discuss the broader implications of our findings for bioinformatics, particularly in genomic data processing and analysis. the results of our study pave the way for more efficient and scalable dna sequence classification methods, offering significant potential for advancements in genomic research and applications.","thu, 25 jan 2024 09:17:19 utc (443 kb)"
"15","pretraining and the lasso","erin craig, mert pilanci, thomas le menestrel, balasubramanian narasimhan, manuel rivas, roozbeh dehghannasiri, julia salzman, jonathan taylor, robert tibshirani","methodology (stat.me)","pretraining is a popular and powerful paradigm in machine learning. as an example, suppose one has a modest-sized dataset of images of cats and dogs, and plans to fit a deep neural network to classify them from the pixel features. with pretraining, we start with a neural network trained on a large corpus of images, consisting of not just cats and dogs but hundreds of other image types. then we fix all of the network weights except for the top layer (which makes the final classification) and train (or ""fine tune"") those weights on our dataset. this often results in dramatically better performance than the network trained solely on our smaller dataset. in this paper, we ask the question ""can pretraining help the lasso?"". we develop a framework for the lasso in which an overall model is fit to a large set of data, and then fine-tuned to a specific task on a smaller dataset. this latter dataset can be a subset of the original dataset, but does not need to be. we find that this framework has a wide variety of applications, including stratified models, multinomial targets, multi-response models, conditional average treatment estimation and even gradient boosting. in the stratified model setting, the pretrained lasso pipeline estimates the coefficients common to all groups at the first stage, and then group specific coefficients at the second ""fine-tuning"" stage. we show that under appropriate assumptions, the support recovery rate of the common coefficients is superior to that of the usual lasso trained only on individual groups. this separate identification of common and individual coefficients can also be useful for scientific understanding.","tue, 23 jan 2024 16:59:33 utc (1,781 kb)[v2] thu, 22 feb 2024 06:23:19 utc (1,786 kb)"
"16","embedded hyperspectral band selection with adaptive optimization for image semantic segmentation","yaniv zimmer, oren glickman","computer vision and pattern recognition (cs.cv)","hyperspectral band selection plays a pivotal role in remote sensing and image analysis, aiming to identify the most informative spectral bands while minimizing computational overhead. in this paper, we introduce a pioneering approach for hyperspectral band selection that offers an embedded solution, making it well-suited for resource-constrained or real-time applications. our proposed method, embedded hyperspectral band selection (ehbs), excels in selecting the best bands without the need for prior processing, seamlessly integrating with the downstream task model. this is achieved through the adaptation of the stochastic gates (stg) algorithm, originally designed for feature selection, for hyperspectral band selection in the context of image semantic segmentation and the integration of a dynamic optimizer, dog, which removes the need for the required tuning the learning rate. to assess the performance of our method, we introduce a novel metric for evaluating band selection methods across different target numbers of selected bands quantified by the area under the curve (auc). we conduct experiments on two distinct semantic-segmentation hyperspectral benchmark datasets, demonstrating its superiority in terms of its resulting accuracy and its ease of use compared to many common and state-of-the-art methods. furthermore, our contributions extend beyond the realm of hyperspectral band selection. the adaptability of our approach to other tasks, especially those involving grouped features, opens up promising avenues for broader applications within the realm of deep learning, such as feature selection for feature groups. the demonstrated success on the tested datasets and the potential for application to a variety of tasks underscore the value of our method as a substantial addition to the field of computer vision.","sun, 21 jan 2024 07:48:39 utc (3,120 kb)"
"17","cognitivedog: large multimodal model based system to translate vision and language into action of quadruped robot","artem lykov, mikhail litvinov, mikhail konenkov, rinat prochii, nikita burtsev, ali alridha abdulkarim, artem bazhenov, vladimir berman, dzmitry tsetserukou","robotics (cs.ro)","this paper introduces cognitivedog, a pioneering development of quadruped robot with large multi-modal model (lmm) that is capable of not only communicating with humans verbally but also physically interacting with the environment through object manipulation. the system was realized on unitree go1 robot-dog equipped with a custom gripper and demonstrated autonomous decision-making capabilities, independently determining the most appropriate actions and interactions with various objects to fulfill user-defined tasks. these tasks do not necessarily include direct instructions, challenging the robot to comprehend and execute them based on natural language input and environmental cues. the paper delves into the intricacies of this system, dataset characteristics, and the software architecture. key to this development is the robot's proficiency in navigating space using visual-slam, effectively manipulating and transporting objects, and providing insightful natural language commentary during task execution. experimental results highlight the robot's advanced task comprehension and adaptability, underscoring its potential in real-world applications. the dataset used to fine-tune the robot-dog behavior generation model is provided at the following link: this http url","wed, 17 jan 2024 18:01:24 utc (5,849 kb)"
"18","revalued: regularised ensemble value-decomposition for factorisable markov decision processes","david ireland, giovanni montana","machine learning (cs.lg)","discrete-action reinforcement learning algorithms often falter in tasks with high-dimensional discrete action spaces due to the vast number of possible actions. a recent advancement leverages value-decomposition, a concept from multi-agent reinforcement learning, to tackle this challenge. this study delves deep into the effects of this value-decomposition, revealing that whilst it curtails the over-estimation bias inherent to q-learning algorithms, it amplifies target variance. to counteract this, we present an ensemble of critics to mitigate target variance. moreover, we introduce a regularisation loss that helps to mitigate the effects that exploratory actions in one dimension can have on the value of optimal actions in other dimensions. our novel algorithm, revalued, tested on discretised versions of the deepmind control suite tasks, showcases superior performance, especially in the challenging humanoid and dog tasks. we further dissect the factors influencing revalued's performance, evaluating the significance of the regularisation loss and the scalability of revalued with increasing sub-actions per dimension.","tue, 16 jan 2024 21:47:23 utc (2,102 kb)[v2] fri, 8 mar 2024 10:06:53 utc (2,103 kb)"
"19","dynamics and numerical simulations to predict empirical antibiotic treatment of multi-resistant pseudomonas aeruginosa infection","javier lópez-de-la-cruz, maría pérez-aranda, ana alcudia, belén begines, tomás caraballo, eloísa pajuelo, pedro j. ginel","populations and evolution (q-bio.pe)","this work discloses an epidemiological mathematical model to predict an empirical treatment for dogs infected by pseudomonas aeruginosa. this dangerous pathogen is one of the leading causes of multi-resistant infections and can be transmitted from dogs to humans. numerical simulations and appropriated codes were developed using matlab software to gather information concerning long-time dynamics of the susceptible, infected and recovered individuals. all data compiled from the mathematical model was used to provide an appropriated antibiotic sensitivity panel for this specific infection. in this study, several variables have been included in this model to predict which treatment should be prescribed in emergency cases, when there is no time to perform an antibiogram or the cost of it could not be assumed. in particular, we highlight the use of this model aiming to become part of the convenient toolbox of public health research and decision-making in the design of the mitigation strategy of bacterial pathogens.","sat, 13 jan 2024 00:34:10 utc (1,555 kb)"
"20","benchmark analysis of various pre-trained deep learning models on assira cats and dogs dataset","galib muhammad shahriar himel, md. masudul islam","computer vision and pattern recognition (cs.cv)","as the most basic application and implementation of deep learning, image classification has grown in popularity. various datasets are provided by renowned data science communities for benchmarking machine learning algorithms and pre-trained models. the assira cats & dogs dataset is one of them and is being used in this research for its overall acceptance and benchmark standards. a comparison of various pre-trained models is demonstrated by using different types of optimizers and loss functions. hyper-parameters are changed to gain the best result from a model. by applying this approach, we have got higher accuracy without major changes in the training model. to run the experiment, we used three different computer architectures: a laptop equipped with nvidia geforce gtx 1070, a laptop equipped with nvidia geforce rtx 3080ti, and a desktop equipped with nvidia geforce rtx 3090. the acquired results demonstrate supremacy in terms of accuracy over the previously done experiments on this dataset. from this experiment, the highest accuracy which is 99.65% is gained using the nasnet large.","tue, 9 jan 2024 16:48:11 utc (950 kb)"
"21","emogen: emotional image content generation with text-to-image diffusion models","jingyuan yang, jiawei feng, hui huang","computer vision and pattern recognition (cs.cv)","recent years have witnessed remarkable progress in image generation task, where users can create visually astonishing images with high-quality. however, existing text-to-image diffusion models are proficient in generating concrete concepts (dogs) but encounter challenges with more abstract ones (emotions). several efforts have been made to modify image emotions with color and style adjustments, facing limitations in effectively conveying emotions with fixed image contents. in this work, we introduce emotional image content generation (eicg), a new task to generate semantic-clear and emotion-faithful images given emotion categories. specifically, we propose an emotion space and construct a mapping network to align it with the powerful contrastive language-image pre-training (clip) space, providing a concrete interpretation of abstract emotions. attribute loss and emotion confidence are further proposed to ensure the semantic diversity and emotion fidelity of the generated images. our method outperforms the state-of-the-art text-to-image approaches both quantitatively and qualitatively, where we derive three custom metrics, i.e., emotion accuracy, semantic clarity and semantic diversity. in addition to generation, our method can help emotion understanding and inspire emotional art design.","tue, 9 jan 2024 15:23:21 utc (26,617 kb)"
"22","pushing boundaries: exploring zero shot object classification with large multimodal models","ashhadul islam, md. rafiul biswas, wajdi zaghouani, samir brahim belhaouari, zubair shah","computer vision and pattern recognition (cs.cv)","$ $the synergy of language and vision models has given rise to large language and vision assistant models (llvas), designed to engage users in rich conversational experiences intertwined with image-based queries. these comprehensive multimodal models seamlessly integrate vision encoders with large language models (llms), expanding their applications in general-purpose language and visual comprehension. the advent of large multimodal models (lmms) heralds a new era in artificial intelligence (ai) assistance, extending the horizons of ai utilization. this paper takes a unique perspective on lmms, exploring their efficacy in performing image classification tasks using tailored prompts designed for specific datasets. we also investigate the llvas zero-shot learning capabilities. our study includes a benchmarking analysis across four diverse datasets: mnist, cats vs. dogs, hymnoptera (ants vs. bees), and an unconventional dataset comprising pox vs. non-pox skin images. the results of our experiments demonstrate the model's remarkable performance, achieving classification accuracies of 85\%, 100\%, 77\%, and 79\% for the respective datasets without any fine-tuning. to bolster our analysis, we assess the model's performance post fine-tuning for specific tasks. in one instance, fine-tuning is conducted over a dataset comprising images of faces of children with and without autism. prior to fine-tuning, the model demonstrated a test accuracy of 55\%, which significantly improved to 83\% post fine-tuning. these results, coupled with our prior findings, underscore the transformative potential of llvas and their versatile applications in real-world scenarios.","sat, 30 dec 2023 03:19:54 utc (5,262 kb)"
"23","a proposed new metric for the conceptual diversity of a text","ilknur dönmez phd, mehmet haklıdır phd","computation and language (cs.cl)","a word may contain one or more hidden concepts. while the ""animal"" word evokes many images in our minds and encapsulates many concepts (birds, dogs, cats, crocodiles, etc.), the `parrot' word evokes a single image (a colored bird with a short, hooked beak and the ability to mimic sounds). in spoken or written texts, we use some words in a general sense and some in a detailed way to point to a specific object. until now, a text's conceptual diversity value cannot be determined using a standard and precise technique. this research contributes to the natural language processing field of ai by offering a standardized method and a generic metric for evaluating and comparing concept diversity in different texts and domains. it also contributes to the field of semantic research of languages. if we give examples for the diversity score of two sentences, ""he discovered an unknown entity."" has a high conceptual diversity score (16.6801), and ""the endoplasmic reticulum forms a series of flattened sacs within the cytoplasm of eukaryotic cells."" sentence has a low conceptual diversity score which is 3.9068.","wed, 27 dec 2023 12:19:06 utc (705 kb)"
"24","quadattack: a quadratic programming approach to ordered top-k attacks","thomas paniagua, ryan grainger, tianfu wu","cryptography and security (cs.cr)","the adversarial vulnerability of deep neural networks (dnns) has been well-known and widely concerned, often under the context of learning top-$1$ attacks (e.g., fooling a dnn to classify a cat image as dog). this paper shows that the concern is much more serious by learning significantly more aggressive ordered top-$k$ clear-box~\footnote{ this is often referred to as white/black-box attacks in the literature. we choose to adopt neutral terminology, clear/opaque-box attacks in this paper, and omit the prefix clear-box for simplicity.} targeted attacks proposed in adversarial distillation. we propose a novel and rigorous quadratic programming (qp) method of learning ordered top-$k$ attacks with low computing cost, dubbed as \textbf{quadattac$k$}. our quadattac$k$ directly solves the qp to satisfy the attack constraint in the feature embedding space (i.e., the input space to the final linear classifier), which thus exploits the semantics of the feature embedding space (i.e., the principle of class coherence). with the optimized feature embedding vector perturbation, it then computes the adversarial perturbation in the data space via the vanilla one-step back-propagation. in experiments, the proposed quadattac$k$ is tested in the imagenet-1k classification using resnet-50, densenet-121, and vision transformers (vit-b and deit-s). it successfully pushes the boundary of successful ordered top-$k$ attacks from $k=10$ up to $k=20$ at a cheap budget ($1\times 60$) and further improves attack success rates for $k=5$ for all tested models, while retaining the performance for $k=1$.","tue, 12 dec 2023 05:08:45 utc (22,472 kb)"
"25","keep the faith: faithful explanations in convolutional neural networks for case-based reasoning","tom nuno wolf, fabian bongratz, anne-marie rickmann, sebastian pölsterl, christian wachinger","machine learning (cs.lg)","explaining predictions of black-box neural networks is crucial when applied to decision-critical tasks. thus, attribution maps are commonly used to identify important image regions, despite prior work showing that humans prefer explanations based on similar examples. to this end, protopnet learns a set of class-representative feature vectors (prototypes) for case-based reasoning. during inference, similarities of latent features to prototypes are linearly classified to form predictions and attribution maps are provided to explain the similarity. in this work, we evaluate whether architectures for case-based reasoning fulfill established axioms required for faithful explanations using the example of protopnet. we show that such architectures allow the extraction of faithful explanations. however, we prove that the attribution maps used to explain the similarities violate the axioms. we propose a new procedure to extract explanations for trained protopnets, named protopfaith. conceptually, these explanations are shapley values, calculated on the similarity scores of each prototype. they allow to faithfully answer which prototypes are present in an unseen image and quantify each pixel's contribution to that presence, thereby complying with all axioms. the theoretical violations of protopnet manifest in our experiments on three datasets (cub-200-2011, stanford dogs, rsna) and five architectures (convnet, resnet, resnet50, wideresnet50, resnext50). our experiments show a qualitative difference between the explanations given by protopnet and protopfaith. additionally, we quantify the explanations with the area over the perturbation curve, on which protopfaith outperforms protopnet on all experiments by a factor $>10^3$.","fri, 15 dec 2023 13:36:54 utc (5,738 kb)[v2] mon, 18 dec 2023 17:13:15 utc (5,738 kb)[v3] tue, 19 dec 2023 10:36:41 utc (5,739 kb)"
"26","toward open-ended embodied tasks solving","william wei wang, dongqi han, xufang luo, yifei shen, charles ling, boyu wang, dongsheng li","artificial intelligence (cs.ai)","empowering embodied agents, such as robots, with artificial intelligence (ai) has become increasingly important in recent years. a major challenge is task open-endedness. in practice, robots often need to perform tasks with novel goals that are multifaceted, dynamic, lack a definitive ""end-state"", and were not encountered during training. to tackle this problem, this paper introduces \textit{diffusion for open-ended goals} (dog), a novel framework designed to enable embodied ai to plan and act flexibly and dynamically for open-ended task goals. dog synergizes the generative prowess of diffusion models with state-of-the-art, training-free guidance techniques to adaptively perform online planning and control. our evaluations demonstrate that dog can handle various kinds of novel task goals not seen during training, in both maze navigation and robot control problems. our work sheds light on enhancing embodied ai's adaptability and competency in tackling open-ended goals.","sun, 10 dec 2023 08:43:26 utc (4,376 kb)"
"27","model evaluation for domain identification of unknown classes in open-world recognition: a proposal","gusti ahmad fanshuri alfarisy, owais ahmed malik, ong wee hong","computer vision and pattern recognition (cs.cv)","open-world recognition (owr) is an emerging field that makes a machine learning model competent in rejecting the unknowns, managing them, and incrementally adding novel samples to the base knowledge. however, this broad objective is not practical for an agent that works on a specific task. not all rejected samples will be used for learning continually in the future. some novel images in the open environment may not belong to the domain of interest. hence, identifying the unknown in the domain of interest is essential for a machine learning model to learn merely the important samples. in this study, we propose an evaluation protocol for estimating a model's capability in separating unknown in-domain (id) and unknown out-of-domain (ood). we evaluated using three approaches with an unknown domain and demonstrated the possibility of identifying the domain of interest using the pre-trained parameters through traditional transfer learning, automated machine learning (automl), and nearest class mean (ncm) classifier with first integer neighbor clustering hierarchy (finch). we experimented with five different domains: garbage, food, dogs, plants, and birds. the results show that all approaches can be used as an initial baseline yielding a good accuracy. in addition, a balanced accuracy (baccu) score from a pre-trained model indicates a tendency to excel in one or more domains of interest. we observed that mobilenetv3 yielded the highest baccu score for the garbage domain and surpassed complex models such as the transformer network. meanwhile, our results also suggest that a strong representation in the pre-trained model is important for identifying unknown classes in the same domain. this study could open the bridge toward open-world recognition in domain-specific tasks where the relevancy of the unknown classes is vital.","sat, 9 dec 2023 03:54:25 utc (597 kb)"
"28","human-readable fingerprint for large language models","boyi zeng, chenghu zhou, xinbing wang, zhouhan lin","computation and language (cs.cl)","protecting the copyright of large language models (llms) has become crucial due to their resource-intensive training and accompanying carefully designed licenses. however, identifying the original base model of an llm is challenging due to potential parameter alterations. in this study, we introduce a human-readable fingerprint for llms that uniquely identifies the base model without exposing model parameters or interfering with training. we first observe that the vector direction of llm parameters remains stable after the model has converged during pretraining, showing negligible perturbations through subsequent training steps, including continued pretraining, supervised fine-tuning (sft), and rlhf, which makes it a sufficient condition to identify the base model. the necessity is validated by continuing to train an llm with an extra term to drive away the model parameters' direction and the model becomes damaged. however, this direction is vulnerable to simple attacks like dimension permutation or matrix rotation, which significantly change it without affecting performance. to address this, leveraging the transformer structure, we systematically analyze potential attacks and define three invariant terms that identify an llm's base model. we make these invariant terms human-readable by mapping them to a gaussian vector using a convolutional encoder and then converting it into a natural image with stylegan2. our method generates a dog image as an identity fingerprint for an llm, where the dog's appearance strongly indicates the llm's base model. the fingerprint provides intuitive information for qualitative discrimination, while the invariant terms can be employed for quantitative and precise verification. experimental results across various llms demonstrate the effectiveness of our method.","fri, 8 dec 2023 05:01:47 utc (5,257 kb)[v2] wed, 7 feb 2024 11:01:25 utc (9,583 kb)"
"29","impact of data augmentation on qcnns","leting zhouli, peiyong wang, udaya parampalli","quantum physics (quant-ph)","in recent years, classical convolutional neural networks (cnns) have been applied for image recognition successfully. quantum convolutional neural networks (qcnns) are proposed as a novel generalization to cnns by using quantum mechanisms. the quantum mechanisms lead to an efficient training process in qcnns by reducing the size of input from $n$ to $log_2n$. this paper implements and compares both cnns and qcnns by testing losses and prediction accuracy on three commonly used datasets. the datasets include the mnist hand-written digits, fashion mnist and cat/dog face images. additionally, data augmentation (da), a technique commonly used in cnns to improve the performance of classification by generating similar images based on original inputs, is also implemented in qcnns. surprisingly, the results showed that data augmentation didn't improve qcnns performance. the reasons and logic behind this result are discussed, hoping to expand our understanding of quantum machine learning theory.","fri, 1 dec 2023 05:28:19 utc (830 kb)"
"30","mixture of gaussian-distributed prototypes with generative modelling for interpretable image classification","chong wang, yuanhong chen, fengbei liu, davis james mccarthy, helen frazer, gustavo carneiro","computer vision and pattern recognition (cs.cv)","prototypical-part interpretable methods, e.g., protopnet, enhance interpretability by connecting classification predictions to class-specific training prototypes, thereby offering an intuitive insight into their decision-making. current methods rely on a discriminative classifier trained with point-based learning techniques that provide specific values for prototypes. such prototypes have relatively low representation power due to their sparsity and potential redundancy, with each prototype containing no variability measure. in this paper, we present a new generative learning of prototype distributions, named mixture of gaussian-distributed prototypes (mgproto), which are represented by gaussian mixture models (gmm). such an approach enables the learning of more powerful prototype representations since each learned prototype will own a measure of variability, which naturally reduces the sparsity given the spread of the distribution around each prototype, and we also integrate a prototype diversity objective function into the gmm optimisation to reduce redundancy. incidentally, the generative nature of mgproto offers a new and effective way for detecting out-of-distribution samples. to improve the compactness of mgproto, we further propose to prune gaussian-distributed prototypes with a low prior. experiments on cub-200-2011, stanford cars, stanford dogs, and oxford-iiit pets datasets show that mgproto achieves state-of-the-art classification and ood detection performances with encouraging interpretability results.","thu, 30 nov 2023 11:01:37 utc (6,198 kb)"
"31","unlocking spatial comprehension in text-to-image diffusion models","mohammad mahdi derakhshani, menglin xia, harkirat behl, cees g. m. snoek, victor rühle","computer vision and pattern recognition (cs.cv)","we propose compfuser, an image generation pipeline that enhances spatial comprehension and attribute assignment in text-to-image generative models. our pipeline enables the interpretation of instructions defining spatial relationships between objects in a scene, such as `an image of a gray cat on the left of an orange dog', and generate corresponding images. this is especially important in order to provide more control to the user. compfuser overcomes the limitation of existing text-to-image diffusion models by decoding the generation of multiple objects into iterative steps: first generating a single object and then editing the image by placing additional objects in their designated positions. to create training data for spatial comprehension and attribute assignment we introduce a synthetic data generation process, that leverages a frozen large language model and a frozen layout-based diffusion model for object placement. we compare our approach to strong baselines and show that our model outperforms state-of-the-art image generation models in spatial comprehension and attribute assignment, despite being 3x to 5x smaller in parameters.","tue, 28 nov 2023 19:00:02 utc (24,097 kb)"
"32","space-time diffusion features for zero-shot text-driven motion transfer","danah yatim, rafail fridman, omer bar-tal, yoni kasten, tali dekel","computer vision and pattern recognition (cs.cv)","we present a new method for text-driven motion transfer - synthesizing a video that complies with an input text prompt describing the target objects and scene while maintaining an input video's motion and scene layout. prior methods are confined to transferring motion across two subjects within the same or closely related object categories and are applicable for limited domains (e.g., humans). in this work, we consider a significantly more challenging setting in which the target and source objects differ drastically in shape and fine-grained motion characteristics (e.g., translating a jumping dog into a dolphin). to this end, we leverage a pre-trained and fixed text-to-video diffusion model, which provides us with generative and motion priors. the pillar of our method is a new space-time feature loss derived directly from the model. this loss guides the generation process to preserve the overall motion of the input video while complying with the target object in terms of shape and fine-grained motion traits.","tue, 28 nov 2023 18:03:27 utc (46,867 kb)[v2] sun, 3 dec 2023 12:30:05 utc (46,867 kb)"
"33","dreamcreature: crafting photorealistic virtual creatures from imagination","kam woh ng, xiatian zhu, yi-zhe song, tao xiang","computer vision and pattern recognition (cs.cv)","recent text-to-image (t2i) generative models allow for high-quality synthesis following either text instructions or visual examples. despite their capabilities, these models face limitations in creating new, detailed creatures within specific categories (e.g., virtual dog or bird species), which are valuable in digital asset creation and biodiversity analysis. to bridge this gap, we introduce a novel task, virtual creatures generation: given a set of unlabeled images of the target concepts (e.g., 200 bird species), we aim to train a t2i model capable of creating new, hybrid concepts within diverse backgrounds and contexts. we propose a new method called dreamcreature, which identifies and extracts the underlying sub-concepts (e.g., body parts of a specific species) in an unsupervised manner. the t2i thus adapts to generate novel concepts (e.g., new bird species) with faithful structures and photorealistic appearance by seamlessly and flexibly composing learned sub-concepts. to enhance sub-concept fidelity and disentanglement, we extend the textual inversion technique by incorporating an additional projector and tailored attention loss regularization. extensive experiments on two fine-grained image benchmarks demonstrate the superiority of dreamcreature over prior methods in both qualitative and quantitative evaluation. ultimately, the learned sub-concepts facilitate diverse creative applications, including innovative consumer product designs and nuanced property modifications.","mon, 27 nov 2023 01:24:31 utc (18,791 kb)"
"34","quantum learning and essential cognition under the traction of meta-characteristics in an open world","jin wang, changlin song","artificial intelligence (cs.ai)","artificial intelligence has made significant progress in the close world problem, being able to accurately recognize old knowledge through training and classification. however, ai faces significant challenges in the open world problem, as it involves a new and unknown exploration journey. ai is not inherently proactive in exploration, and its challenge lies in not knowing how to approach and adapt to the unknown world. how do humans acquire knowledge of the unknown world. humans identify new knowledge through intrinsic cognition. in the process of recognizing new colors, the cognitive cues are different from known color features and involve hue, saturation, brightness, and other characteristics. when ai encounters objects with different features in the new world, it faces another challenge: where are the distinguishing features between influential features of new and old objects? ai often mistakes a new world's brown bear for a known dog because it has not learned the differences in feature distributions between knowledge systems. this is because things in the new and old worlds have different units and dimensions for their features. this paper proposes an open-world model and elemental feature system that focuses on fundamentally recognizing the distribution differences in objective features between the new and old worlds. the quantum tunneling effect of learning ability in the new and old worlds is realized through the tractive force of meta-characteristic. the outstanding performance of the model system in learning new knowledge (using pedestrian re-identification datasets as an example) demonstrates that ai has acquired the ability to recognize the new world with an accuracy of $96.71\%$ at most and has gained the capability to explore new knowledge, similar to humans.","wed, 22 nov 2023 11:55:41 utc (11,284 kb)"
"35","das: a deformable attention to capture salient information in cnns","farzad salajegheh, nader asadi, soroush saryazdi, sudhir mudur","computer vision and pattern recognition (cs.cv)","convolutional neural networks (cnns) excel in local spatial pattern recognition. for many vision tasks, such as object recognition and segmentation, salient information is also present outside cnn's kernel boundaries. however, cnns struggle in capturing such relevant information due to their confined receptive fields. self-attention can improve a model's access to global information but increases computational overhead. we present a fast and simple fully convolutional method called das that helps focus attention on relevant information. it uses deformable convolutions for the location of pertinent image regions and separable convolutions for efficiency. das plugs into existing cnns and propagates relevant information using a gating mechanism. compared to the o(n^2) computational complexity of transformer-style attention, das is o(n). our claim is that das's ability to pay increased attention to relevant features results in performance improvements when added to popular cnns for image classification and object detection. for example, das yields an improvement on stanford dogs (4.47%), imagenet (1.91%), and coco ap (3.3%) with base resnet50 backbone. this outperforms other cnn attention mechanisms while using similar or less flops. our code will be publicly available.","mon, 20 nov 2023 18:49:58 utc (13,040 kb)"
"36","categorizing the visual environment and analyzing the visual attention of dogs","shreyas sundara raman, madeline h. pelgrim, daphna buchsbaum, thomas serre","computer vision and pattern recognition (cs.cv)","dogs have a unique evolutionary relationship with humans and serve many important roles e.g. search and rescue, blind assistance, emotional support. however, few datasets exist to categorize visual features and objects available to dogs, as well as how dogs direct their visual attention within their environment. we collect and study a dataset with over 11,698 gazes to categorize the objects available to be gazed at by 11 dogs in everyday outdoor environments i.e. a walk around a college campus and urban area. we explore the availability of these object categories and the visual attention of dogs over these categories using a head mounted eye tracking apparatus. a small portion (approx. 600 images or < 20% of total dataset) of the collected data is used to fine tune a maskrcnn for the novel image domain to segment objects present in the scene, enabling further statistical analysis on the visual gaze tendencies of dogs. the maskrcnn, with eye tracking apparatus, serves as an end to end model for automatically classifying the visual fixations of dogs. the fine tuned maskrcnn performs far better than chance. there are few individual differences between the 11 dogs and we observe greater visual fixations on buses, plants, pavement, and construction equipment. this work takes a step towards understanding visual behavior of dogs and their interaction with the physical world.","mon, 20 nov 2023 18:21:18 utc (5,248 kb)"
"37","data center audio/video intelligence on device (david) -- an edge-ai platform for smart-toys","gabriel cosache, francisco salgado, cosmin rotariu, george sterpu, rishabh jain, peter corcoran","artificial intelligence (cs.ai)","an overview is given of the david smart-toy platform, one of the first edge ai platform designs to incorporate advanced low-power data processing by neural inference models co-located with the relevant image or audio sensors. there is also on-board capability for in-device text-to-speech generation. two alternative embodiments are presented: a smart teddy-bear, and a roving dog-like robot. the platform offers a speech-driven user interface and can observe and interpret user actions and facial expressions via its computer vision sensor node. a particular benefit of this design is that no personally identifiable information passes beyond the neural inference nodes thus providing inbuilt compliance with data protection regulations.","sat, 18 nov 2023 10:38:35 utc (3,375 kb)"
"38","neural networks are implicit decision trees: the hierarchical simplicity bias","zhehang du","machine learning (cs.lg)","neural networks exhibit simplicity bias; they rely on simpler features while ignoring equally predictive but more complex features. in this work, we introduce a novel approach termed imbalanced label coupling to investigate scenarios where simple and complex features exhibit different levels of predictive power. in these cases, complex features still contribute to predictions. the trained networks make predictions in alignment with the ascending complexity of input features according to how they correlate with the label in the training set, irrespective of the underlying predictive power. for instance, even when simple spurious features distort predictions in cifar-10, most cats are predicted to be dogs, and most trucks are predicted to be automobiles! this observation provides direct evidence that the neural network learns core features in the presence of spurious features. we empirically show that last-layer retraining with target data distribution is effective, yet insufficient to fully recover core features when spurious features are perfectly correlated with the target labels in our synthetic dataset. we hope our research contributes to a deeper understanding of the implicit bias of neural networks.","sun, 5 nov 2023 11:27:03 utc (580 kb)"
"39","see sift in a rain","wei wu, hao chang, zhu li","image and video processing (eess.iv)","rain streaks bring complicated pixel intensity changes and additional gradients, greatly obstructing the extraction of image features from background. this causes serious performance degradation in feature-based applications. thus, it is critical to remove rain streaks from a single rainy image to recover image features. recently, many excellent image deraining methods have made remarkable progress. however, these human visual system-driven approaches mainly focus on improving image quality with pixel recovery as loss function, and neglect how to enhance image feature recovery ability. to address this issue, we propose a task-driven image deraining algorithm to strengthen image feature supply for subsequent feature-based applications. due to the extensive use and strong practicability of scale-invariant feature transform (sift), we first propose two separate networks using distinct losses and modules to achieve two goals, respectively. one is difference of gaussian (dog) pyramid recovery network (dprnet) for sift detection, and the other gradients of gaussian images recovery network (ggirnet) for sift description. second, in the dprnet we propose an alternative interest point loss that directly penalizes scale response extrema to recover the dog pyramid. third, we advance a gradient attention module in the ggirnet to recover those gradients of gaussian images. finally, with the recovered dog pyramid and gradients, we can regain sift key points. this divide-and-conquer scheme to set different objectives for sift detection and description leads to good robustness. compared with state-of-the-art methods, experimental results demonstrate that our proposed algorithm achieves better performance in both the number of recovered sift key points and their accuracy.","wed, 1 nov 2023 13:42:23 utc (1,959 kb)"
"40","what's ""up"" with vision-language models? investigating their struggle with spatial reasoning","amita kamath, jack hessel, kai-wei chang","computation and language (cs.cl)","recent vision-language (vl) models are powerful, but can they reliably distinguish ""right"" from ""left""? we curate three new corpora to quantify model comprehension of such basic spatial relations. these tests isolate spatial reasoning more precisely than existing datasets like vqav2, e.g., our what'sup benchmark contains sets of photographs varying only the spatial relations of objects, keeping their identity fixed (see figure 1: models must comprehend not only the usual case of a dog under a table, but also, the same dog on top of the same table). we evaluate 18 vl models, finding that all perform poorly, e.g., blip finetuned on vqav2, which nears human parity on vqav2, achieves 56% accuracy on our benchmarks vs. humans at 99%. we conclude by studying causes of this surprising behavior, finding: 1) that popular vision-language pretraining corpora like laion-2b contain little reliable data for learning spatial relationships; and 2) that basic modeling interventions like up-weighting preposition-containing instances or fine-tuning on our corpora are not sufficient to address the challenges our benchmarks pose. we are hopeful that these corpora will facilitate further research, and we release our data and code at this https url.","mon, 30 oct 2023 17:50:15 utc (13,534 kb)"
"41","drm: mastering visual reinforcement learning through dormant ratio minimization","guowei xu, ruijie zheng, yongyuan liang, xiyao wang, zhecheng yuan, tianying ji, yu luo, xiaoyu liu, jiaxin yuan, pu hua, shuzhen li, yanjie ze, hal daumé iii, furong huang, huazhe xu","machine learning (cs.lg)","visual reinforcement learning (rl) has shown promise in continuous control tasks. despite its progress, current algorithms are still unsatisfactory in virtually every aspect of the performance such as sample efficiency, asymptotic performance, and their robustness to the choice of random seeds. in this paper, we identify a major shortcoming in existing visual rl methods that is the agents often exhibit sustained inactivity during early training, thereby limiting their ability to explore effectively. expanding upon this crucial observation, we additionally unveil a significant correlation between the agents' inclination towards motorically inactive exploration and the absence of neuronal activity within their policy networks. to quantify this inactivity, we adopt dormant ratio as a metric to measure inactivity in the rl agent's network. empirically, we also recognize that the dormant ratio can act as a standalone indicator of an agent's activity level, regardless of the received reward signals. leveraging the aforementioned insights, we introduce drm, a method that uses three core mechanisms to guide agents' exploration-exploitation trade-offs by actively minimizing the dormant ratio. experiments demonstrate that drm achieves significant improvements in sample efficiency and asymptotic performance with no broken seeds (76 seeds in total) across three continuous control benchmark environments, including deepmind control suite, metaworld, and adroit. most importantly, drm is the first model-free algorithm that consistently solves tasks in both the dog and manipulator domains from the deepmind control suite as well as three dexterous hand manipulation tasks without demonstrations in adroit, all based on pixel observations.","mon, 30 oct 2023 15:50:56 utc (3,716 kb)[v2] wed, 14 feb 2024 03:56:25 utc (3,582 kb)"
"42","on the theory of risk-aware agents: bridging actor-critic and economics","michal nauman, marek cygan","machine learning (cs.lg)","risk-aware reinforcement learning (rl) algorithms like sac and td3 were shown empirically to outperform their risk-neutral counterparts in a variety of continuous-action tasks. however, the theoretical basis for the pessimistic objectives these algorithms employ remains unestablished, raising questions about the specific class of policies they are implementing. in this work, we apply the expected utility hypothesis, a fundamental concept in economics, to illustrate that both risk-neutral and risk-aware rl goals can be interpreted through expected utility maximization using an exponential utility function. this approach reveals that risk-aware policies effectively maximize value certainty equivalent, aligning them with conventional decision theory principles. furthermore, we propose dual actor-critic (dac). dac is a risk-aware, model-free algorithm that features two distinct actor networks: a pessimistic actor for temporal-difference learning and an optimistic actor for exploration. our evaluations of dac across various locomotion and manipulation tasks demonstrate improvements in sample efficiency and final performance. remarkably, dac, while requiring significantly less computational resources, matches the performance of leading model-based methods in the complex dog and humanoid domains.","mon, 30 oct 2023 13:28:06 utc (10,288 kb)[v2] sat, 2 mar 2024 12:40:15 utc (34,597 kb)"
"43","attribute based interpretable evaluation metrics for generative models","dongkyun kim, mingi kwon, youngjung uh","computer vision and pattern recognition (cs.cv)","when the training dataset comprises a 1:1 proportion of dogs to cats, a generative model that produces 1:1 dogs and cats better resembles the training species distribution than another model with 3:1 dogs and cats. can we capture this phenomenon using existing metrics? unfortunately, we cannot, because these metrics do not provide any interpretability beyond ""diversity"". in this context, we propose a new evaluation protocol that measures the divergence of a set of generated images from the training set regarding the distribution of attribute strengths as follows. single-attribute divergence (sad) measures the divergence regarding pdfs of a single attribute. paired-attribute divergence (pad) measures the divergence regarding joint pdfs of a pair of attributes. they provide which attributes the models struggle. for measuring the attribute strengths of an image, we propose heterogeneous clipscore (hcs) which measures the cosine similarity between image and text vectors with heterogeneous initial points. with sad and pad, we reveal the following about existing generative models. projectedgan generates implausible attribute relationships such as a baby with a beard even though it has competitive scores of existing metrics. diffusion models struggle to capture diverse colors in the datasets. the larger sampling timesteps of latent diffusion model generate the more minor objects including earrings and necklaces. stable diffusion v1.5 better captures the attributes than v2.1. our metrics lay a foundation for explainable evaluations of generative models.","thu, 26 oct 2023 09:25:09 utc (28,659 kb)"
"44","federated quantum machine learning with differential privacy","rod rofougaran, shinjae yoo, huan-hsin tseng, samuel yen-chi chen","quantum physics (quant-ph)","the preservation of privacy is a critical concern in the implementation of artificial intelligence on sensitive training data. there are several techniques to preserve data privacy but quantum computations are inherently more secure due to the no-cloning theorem, resulting in a most desirable computational platform on top of the potential quantum advantages. there have been prior works in protecting data privacy by quantum federated learning (qfl) and quantum differential privacy (qdp) studied independently. however, to the best of our knowledge, no prior work has addressed both qfl and qdp together yet. here, we propose to combine these privacy-preserving methods and implement them on the quantum platform, so that we can achieve comprehensive protection against data leakage (qfl) and model inversion attacks (qdp). this implementation promises more efficient and secure artificial intelligence. in this paper, we present a successful implementation of these privacy-preservation methods by performing the binary classification of the cats vs dogs dataset. using our quantum-classical machine learning model, we obtained a test accuracy of over 0.98, while maintaining epsilon values less than 1.3. we show that federated differentially private training is a viable privacy preservation method for quantum machine learning on noisy intermediate-scale quantum (nisq) devices.","tue, 10 oct 2023 19:52:37 utc (2,090 kb)"
"45","blind dates: examining the expression of temporality in historical photographs","alexandra barancová, melvin wevers, nanne van noord","computer vision and pattern recognition (cs.cv)","this paper explores the capacity of computer vision models to discern temporal information in visual content, focusing specifically on historical photographs. we investigate the dating of images using openclip, an open-source implementation of clip, a multi-modal language and vision model. our experiment consists of three steps: zero-shot classification, fine-tuning, and analysis of visual content. we use the \textit{de boer scene detection} dataset, containing 39,866 gray-scale historical press photographs from 1950 to 1999. the results show that zero-shot classification is relatively ineffective for image dating, with a bias towards predicting dates in the past. fine-tuning openclip with a logistic classifier improves performance and eliminates the bias. additionally, our analysis reveals that images featuring buses, cars, cats, dogs, and people are more accurately dated, suggesting the presence of temporal markers. the study highlights the potential of machine learning models like openclip in dating images and emphasizes the importance of fine-tuning for accurate temporal analysis. future research should explore the application of these findings to color photographs and diverse datasets.","tue, 10 oct 2023 13:51:24 utc (1,180 kb)"
"46","towards lexical analysis of dog vocalizations via online videos","yufei wang, chunhao zhang, jieyi huang, mengyue wu, kenny zhu","sound (cs.sd)","deciphering the semantics of animal language has been a grand challenge. this study presents a data-driven investigation into the semantics of dog vocalizations via correlating different sound types with consistent semantics. we first present a new dataset of shiba inu sounds, along with contextual information such as location and activity, collected from youtube with a well-constructed pipeline. the framework is also applicable to other animal species. based on the analysis of conditioned probability between dog vocalizations and corresponding location and activity, we discover supporting evidence for previous heuristic research on the semantic meaning of various dog sounds. for instance, growls can signify interactions. furthermore, our study yields new insights that existing word types can be subdivided into finer-grained subtypes and minimal semantic unit for shiba inu is word-related. for example, whimper can be subdivided into two types, attention-seeking and discomfort.","thu, 21 sep 2023 23:53:14 utc (28,992 kb)"
"47","does my dog ''speak'' like me? the acoustic correlation between pet dogs and their human owners","jieyi huang, chunhao zhang, yufei wang, mengyue wu, kenny zhu","sound (cs.sd)","how hosts language influence their pets' vocalization is an interesting yet underexplored problem. this paper presents a preliminary investigation into the possible correlation between domestic dog vocal expressions and their human host's language environment. we first present a new dataset of shiba inu dog vocals from youtube, which provides 7500 clean sound clips, including their contextual information of these vocals and their owner's speech clips with a carefully-designed data processing pipeline. the contextual information includes the scene category in which the vocal was recorded, the dog's location and activity. with a classification task and prominent factor analysis, we discover significant acoustic differences in the dog vocals from the two language environments. we further identify some acoustic features from dog vocalizations that are potentially correlated to their host language patterns.","thu, 21 sep 2023 23:49:21 utc (14,599 kb)"
"48","visible and nir image fusion algorithm based on information complementarity","zhuo li, bo li","computer vision and pattern recognition (cs.cv)","visible and near-infrared(nir) band sensors provide images that capture complementary spectral radiations from a scene. and the fusion of the visible and nir image aims at utilizing their spectrum properties to enhance image quality. however, currently visible and nir fusion algorithms cannot well take advantage of spectrum properties, as well as lack information complementarity, which results in color distortion and artifacts. therefore, this paper designs a complementary fusion model from the level of physical signals. first, in order to distinguish between noise and useful information, we use two layers of the weight-guided filter and guided filter to obtain texture and edge layers, respectively. second, to generate the initial visible-nir complementarity weight map, the difference maps of visible and nir are filtered by the extend-dog filter. after that, the significant region of nir night-time compensation guides the initial complementarity weight map by the arctani function. finally, the fusion images can be generated by the complementarity weight maps of visible and nir images, respectively. the experimental results demonstrate that the proposed algorithm can not only well take advantage of the spectrum properties and the information complementarity, but also avoid color unnatural while maintaining naturalness, which outperforms the state-of-the-art.","tue, 19 sep 2023 11:07:24 utc (3,889 kb)"
"49","where do free-ranging dogs rest? a population level study reveals hidden patterns in resting site choice","sourabh biswas, kalyan ghosh, kaushikee sarkar, anindita bhadra","populations and evolution (q-bio.pe)","free-ranging dogs (frds) in human-dominated areas encounter obstacles such as noise, pollution, limited food sources, and anthropogenic disturbance while resting. since frds have survived as a population in india, as in many other parts of the global south for centuries, they provide a unique opportunity to study adaptation of animals to the human-dominated urban landscape. we documented factors impacting resting behaviour and site preferences in three states of india, for 284 dogs, leading to 6047 observations over 3 years. 7 physical parameters of the resting sites, along with the biological factors like mating and pup-rearing and time of day affected their choice of resting sites. the frequency-rank distribution of the unique combinations in which the parameters were selected followed a power law distribution, which suggests underlying biological reasons for the observed preferences. further, 3 of these parameters showed maximum consistency of choice in terms of the sub-parameters selected, explaining 30% of the observations. frds prefer to rest close to their resource sites within the territory, at a place that enabled maximum visibility of the surroundings. they chose such sites in the core of the territory for sleeping. at other times, they chose such sites away from the core, and were less restive, thus allowing for immediate response in case of intrusion or threat. they generally avoided anthropogenic disturbance for sleeping, and preferred areas with shade.incorporating these aspects into urban management plans can promote human-dog cooperation and reduce situations of conflict. we envisage more inclusive urban areas in the future, that can allow for co-existence of the humans and their oldest companions in the commensal relationship that has been maintained for hundreds of generations of dogs in this part of the world.","sat, 16 sep 2023 17:33:40 utc (976 kb)"
"50","seeing-eye quadruped navigation with force responsive locomotion control","david defazio, eisuke hirota, shiqi zhang","robotics (cs.ro)","seeing-eye robots are very useful tools for guiding visually impaired people, potentially producing a huge societal impact given the low availability and high cost of real guide dogs. although a few seeing-eye robot systems have already been demonstrated, none considered external tugs from humans, which frequently occur in a real guide dog setting. in this paper, we simultaneously train a locomotion controller that is robust to external tugging forces via reinforcement learning (rl), and an external force estimator via supervised learning. the controller ensures stable walking, and the force estimator enables the robot to respond to the external forces from the human. these forces are used to guide the robot to the global goal, which is unknown to the robot, while the robot guides the human around nearby obstacles via a local planner. experimental results in simulation and on hardware show that our controller is robust to external forces, and our seeing-eye system can accurately detect force direction. we demonstrate our full seeing-eye robot system on a real quadruped robot with a blindfolded human. the video can be seen at our project page: this https url","fri, 8 sep 2023 15:02:46 utc (1,126 kb)[v2] thu, 12 oct 2023 17:48:22 utc (1,128 kb)"
"51","dense object grounding in 3d scenes","wencan huang, daizong liu, wei hu","computer vision and pattern recognition (cs.cv)","localizing objects in 3d scenes according to the semantics of a given natural language is a fundamental yet important task in the field of multimedia understanding, which benefits various real-world applications such as robotics and autonomous driving. however, the majority of existing 3d object grounding methods are restricted to a single-sentence input describing an individual object, which cannot comprehend and reason more contextualized descriptions of multiple objects in more practical 3d cases. to this end, we introduce a new challenging task, called 3d dense object grounding (3d dog), to jointly localize multiple objects described in a more complicated paragraph rather than a single sentence. instead of naively localizing each sentence-guided object independently, we found that dense objects described in the same paragraph are often semantically related and spatially located in a focused region of the 3d scene. to explore such semantic and spatial relationships of densely referred objects for more accurate localization, we propose a novel stacked transformer based framework for 3d dog, named 3dogsformer. specifically, we first devise a contextual query-driven local transformer decoder to generate initial grounding proposals for each target object. then, we employ a proposal-guided global transformer decoder that exploits the local object features to learn their correlation for further refining initial grounding proposals. extensive experiments on three challenging benchmarks (nr3d, sr3d, and scanrefer) show that our proposed 3dogsformer outperforms state-of-the-art 3d single-object grounding methods and their dense-object variants by significant margins.","tue, 5 sep 2023 13:27:19 utc (3,975 kb)"
"52","similarity between compact extremely red objects discovered with jwst in cosmic dawn and blue-excess dust-obscured galaxies known in cosmic noon","akatoki noboriguchi, akio k. inoue, tohru nagao, yoshiki toba, toru misawa","astrophysics of galaxies (astro-ph.ga)","spatially compact objects with extremely red color in the rest-frame optical to near-infrared (0.4--1 ${\rm \mu m}$) and blue color in the rest-frame ultraviolet (uv; 0.2--0.4 ${\rm \mu m}$) have been discovered at $5 < z < 9$ using the james webb space telescope (jwst). these extremely red objects (jwst-eros) exhibit spectral energy distributions (seds) that are difficult to explain using a single component of either star-forming galaxies or quasars, leading to two-component models in which the blue uv and extremely red optical are explained using less-dusty and dusty spectra of galaxies or quasars, respectively. here, we report the remarkable similarity in seds between jwst-eros and blue-excess dust-obscured galaxies (bludogs) identified at $2 < z < 3$. bludogs are a population of active galactic nuclei (agns) with blackhole masses of $\sim10^{8-9}$ m$_\odot$, which are one order of magnitude larger than those in some jwst-eros. the eddington ratios of bludogs are one or higher, whereas those of jwst-eros are in the range of 0.1--1. therefore, jwst-eros are less massive, less active, and more common counterparts in higher-$z$ of bludogs in cosmic noon. conversely, jwst-eros have a significantly higher fraction of those with blue-excess than dogs. we present the average uv spectra of bludogs as a comparison to jwst-eros and discuss a coherent evolutionary scenario for dusty agn populations.","sat, 2 sep 2023 14:44:34 utc (474 kb)[v2] tue, 5 dec 2023 16:44:31 utc (583 kb)"
"53","causality-based feature importance quantifying methods: pn-fi, ps-fi and pns-fi","shuxian du, yaxiu sun, changyi du","artificial intelligence (cs.ai)","in the current ml field models are getting larger and more complex, and data used for model training are also getting larger in quantity and higher in dimensions. therefore, in order to train better models, and save training time and computational resources, a good feature selection (fs) method in the preprocessing stage is necessary. feature importance (fi) is of great importance since it is the basis of feature selection. therefore, this paper creatively introduces the calculation of pn (the probability of necessity), pn (the probability of sufficiency), and pns (the probability of necessity and sufficiency) of causality into quantifying feature importance and creates 3 new fi measuring methods, pn-fi, which means how much importance a feature has in image recognition tasks, ps-fi that means how much importance a feature has in image generating tasks, and pns-fi which measures both. the main body of this paper is three rcts, with whose results we show how ps-fi, pn-fi, and pns-fi of 3 features, dog nose, dog eyes, and dog mouth are calculated. the experiments show that firstly, fi values are intervals with tight upper and lower bounds. secondly, the feature dog eyes has the most importance while the other two have almost the same. thirdly, the bounds of pns and pn are tighter than the bounds of ps.","mon, 28 aug 2023 10:24:51 utc (1,265 kb)[v2] mon, 18 sep 2023 06:28:41 utc (408 kb)"
"54","adversarial illusions in multi-modal embeddings","tingwei zhang, rishi jha, eugene bagdasaryan, vitaly shmatikov","cryptography and security (cs.cr)","multi-modal embeddings encode texts, images, sounds, videos, etc., into a single embedding space, aligning representations across different modalities (e.g., associate an image of a dog with a barking sound). in this paper, we show that multi-modal embeddings can be vulnerable to an attack we call ""adversarial illusions."" given an image or a sound, an adversary can perturb it to make its embedding close to an arbitrary, adversary-chosen input in another modality. these attacks are cross-modal and targeted: the adversary is free to align any image and any sound with any target of his choice. adversarial illusions exploit proximity in the embedding space and are thus agnostic to downstream tasks and modalities, enabling a wholesale compromise of current and future downstream tasks and modalities not available to the adversary. using imagebind and audioclip embeddings, we demonstrate how adversarially aligned inputs, generated without knowledge of specific downstream tasks, mislead image generation, text generation, zero-shot classification, and audio retrieval. we investigate transferability of illusions across different embeddings and develop a black-box version of our method that we use to demonstrate the first adversarial alignment attack on amazon's commercial, proprietary titan embedding. finally, we analyze countermeasures and evasion attacks.","tue, 22 aug 2023 21:57:22 utc (3,863 kb)[v2] fri, 6 oct 2023 10:58:23 utc (3,622 kb)[v3] sat, 17 feb 2024 02:26:27 utc (15,339 kb)"
"55","digitally-enhanced dog behavioral testing: getting help from the machine","nareed farhat, teddy lazebnik, joke monteny, christel palmyre henri moons, eline wydooghe, dirk van der linden, anna zamansky","human-computer interaction (cs.hc)","the assessment of behavioral traits in dogs is a well-studied challenge due to its many practical applications such as selection for breeding, prediction of working aptitude, chances of being adopted, etc. most methods for assessing behavioral traits are questionnaire or observation-based, which require a significant amount of time, effort and expertise. in addition, these methods are also susceptible to subjectivity and bias, making them less reliable. in this study, we proposed an automated computational approach that may provide a more objective, robust and resource-efficient alternative to current solutions. using part of a stranger test protocol, we tested n=53 dogs for their response to the presence and benign actions of a stranger. dog coping styles were scored by three experts. moreover, data were collected from their handlers using the canine behavioral assessment and research questionnaire (c-barq). an unsupervised clustering of the dogs' trajectories revealed two main clusters showing a significant difference in the stranger-directed fear c-barq factor, as well as a good separation between (sufficiently) relaxed dogs and dogs with excessive behaviors towards strangers based on expert scoring. based on the clustering, we obtained a machine learning classifier for expert scoring of coping styles towards strangers, which reached an accuracy of 78%. we also obtained a regression model predicting c-barq factor scores with varying performance, the best being owner-directed aggression (with a mean average error of 0.108) and excitability (with a mean square error of 0.032). this case study demonstrates a novel paradigm of digitally enhanced canine behavioral testing.","wed, 26 jul 2023 14:48:30 utc (5,659 kb)"
"56","learning gabor texture features for fine-grained recognition","lanyun zhu, tianrun chen, jianxiong yin, simon see, jun liu","computer vision and pattern recognition (cs.cv)","extracting and using class-discriminative features is critical for fine-grained recognition. existing works have demonstrated the possibility of applying deep cnns to exploit features that distinguish similar classes. however, cnns suffer from problems including frequency bias and loss of detailed local information, which restricts the performance of recognizing fine-grained categories. to address the challenge, we propose a novel texture branch as complimentary to the cnn branch for feature extraction. we innovatively utilize gabor filters as a powerful extractor to exploit texture features, motivated by the capability of gabor filters in effectively capturing multi-frequency features and detailed local information. we implement several designs to enhance the effectiveness of gabor filters, including imposing constraints on parameter values and developing a learning method to determine the optimal parameters. moreover, we introduce a statistical feature extractor to utilize informative statistical information from the signals captured by gabor filters, and a gate selection mechanism to enable efficient computation by only considering qualified regions as input for texture extraction. through the integration of features from the gabor-filter-based texture branch and cnn-based semantic branch, we achieve comprehensive information extraction. we demonstrate the efficacy of our method on multiple datasets, including cub-200-2011, na-bird, stanford dogs, and gtos-mobile. state-of-the-art performance is achieved using our approach.","thu, 10 aug 2023 07:28:22 utc (6,129 kb)"
"57","from fake to real: pretraining on balanced synthetic images to prevent bias","maan qraitem, kate saenko, bryan a. plummer","computer vision and pattern recognition (cs.cv)","visual recognition models are prone to learning spurious correlations induced by a biased training set where certain conditions $b$ (\eg, indoors) are over-represented in certain classes $y$ (\eg, big dogs). synthetic data from generative models offers a promising direction to mitigate this issue by augmenting underrepresented conditions in the real dataset. however, this introduces another potential source of bias from generative model artifacts in the synthetic data. indeed, as we will show, prior work uses synthetic data to resolve the model's bias toward $b$, but it doesn't correct the models' bias toward the pair $(b, g)$ where $g$ denotes whether the sample is real or synthetic. thus, the model could simply learn signals based on the pair $(b, g)$ (\eg, synthetic indoors) to make predictions about $y$ (\eg, big dogs). to address this issue, we propose a two-step training pipeline that we call from fake to real (ffr). the first step of ffr pre-trains a model on balanced synthetic data to learn robust representations across subgroups. in the second step, ffr fine-tunes the model on real data using erm or common loss-based bias mitigation methods. by training on real and synthetic data separately, ffr avoids the issue of bias toward signals from the pair $(b, g)$. in other words, synthetic data in the first step provides effective unbiased representations that boosts performance in the second step. indeed, our analysis of high bias setting (99.9\%) shows that ffr improves performance over the state-of-the-art by 7-14\% over three datasets (celeba, utk-face, and spuco animals).","tue, 8 aug 2023 19:52:28 utc (732 kb)[v2] fri, 29 sep 2023 05:32:35 utc (3,132 kb)"
"58","beyond one-hot-encoding: injecting semantics to drive image classifiers","alan perotti, simone bertolotto, eliana pastor, andré panisson","computer vision and pattern recognition (cs.cv)","images are loaded with semantic information that pertains to real-world ontologies: dog breeds share mammalian similarities, food pictures are often depicted in domestic environments, and so on. however, when training machine learning models for image classification, the relative similarities amongst object classes are commonly paired with one-hot-encoded labels. according to this logic, if an image is labelled as 'spoon', then 'tea-spoon' and 'shark' are equally wrong in terms of training loss. to overcome this limitation, we explore the integration of additional goals that reflect ontological and semantic knowledge, improving model interpretability and trustworthiness. we suggest a generic approach that allows to derive an additional loss term starting from any kind of semantic information about the classification label. first, we show how to apply our approach to ontologies and word embeddings, and discuss how the resulting information can drive a supervised learning process. second, we use our semantically enriched loss to train image classifiers, and analyse the trade-offs between accuracy, mistake severity, and learned internal representations. finally, we discuss how this approach can be further exploited in terms of explainability and adversarial robustness. code repository: this https url","tue, 1 aug 2023 15:34:02 utc (5,093 kb)"
"59","stochastic positional embeddings improve masked image modeling","amir bar, florian bordes, assaf shocher, mahmoud assran, pascal vincent, nicolas ballas, trevor darrell, amir globerson, yann lecun","computer vision and pattern recognition (cs.cv)","masked image modeling (mim) is a promising self-supervised learning approach that enables learning from unlabeled images. despite its recent success, learning good representations through mim remains challenging because it requires predicting the right semantic content in accurate locations. for example, given an incomplete picture of a dog, we can guess that there is a tail, but we cannot determine its exact location. in this work, we propose to incorporate location uncertainty into mim by using stochastic positional embeddings (stop). specifically, we condition the model on stochastic masked token positions drawn from a gaussian distribution. stop reduces overfitting to location features and guides the model toward learning features that are more robust to location uncertainties. quantitatively, stop improves downstream mim performance on a variety of downstream tasks, including $+1.7\%$ on imagenet linear probing using vit-b, and $+2.5\%$ for vit-h using $1\%$ of the data.","mon, 31 jul 2023 17:59:08 utc (10,636 kb)[v2] tue, 27 feb 2024 18:59:14 utc (2,527 kb)"
"60","promptstyler: prompt-driven style generation for source-free domain generalization","junhyeong cho, gilhyun nam, sungyeon kim, hunmin yang, suha kwak","computer vision and pattern recognition (cs.cv)","in a joint vision-language space, a text feature (e.g., from ""a photo of a dog"") could effectively represent its relevant image features (e.g., from dog photos). also, a recent study has demonstrated the cross-modal transferability phenomenon of this joint space. from these observations, we propose promptstyler which simulates various distribution shifts in the joint space by synthesizing diverse styles via prompts without using any images to deal with source-free domain generalization. the proposed method learns to generate a variety of style features (from ""a s* style of a"") via learnable style word vectors for pseudo-words s*. to ensure that learned styles do not distort content information, we force style-content features (from ""a s* style of a [class]"") to be located nearby their corresponding content features (from ""[class]"") in the joint vision-language space. after learning style word vectors, we train a linear classifier using synthesized style-content features. promptstyler achieves the state of the art on pacs, vlcs, officehome and domainnet, even though it does not require any images for training.","thu, 27 jul 2023 21:14:46 utc (1,723 kb)[v2] tue, 15 aug 2023 08:30:45 utc (1,723 kb)"
"61","affective natural language generation of event descriptions through fine-grained appraisal conditions","yarik menchaca resendiz, roman klinger","computation and language (cs.cl)","models for affective text generation have shown a remarkable progress, but they commonly rely only on basic emotion theories or valance/arousal values as conditions. this is appropriate when the goal is to create explicit emotion statements (""the kid is happy.""). emotions are, however, commonly communicated implicitly. for instance, the emotional interpretation of an event (""their dog died."") does often not require an explicit emotion statement. in psychology, appraisal theories explain the link between a cognitive evaluation of an event and the potentially developed emotion. they put the assessment of the situation on the spot, for instance regarding the own control or the responsibility for what happens. we hypothesize and subsequently show that including appraisal variables as conditions in a generation framework comes with two advantages. (1) the generation model is informed in greater detail about what makes a specific emotion and what properties it has. this leads to text generation that better fulfills the condition. (2) the variables of appraisal allow a user to perform a more fine-grained control of the generated text, by stating properties of a situation instead of only providing the emotion category. our bart and t5-based experiments with 7 emotions (anger, disgust, fear, guilt, joy, sadness, shame), and 7 appraisals (attention, responsibility, control, circumstance, pleasantness, effort, certainty) show that (1) adding appraisals during training improves the accurateness of the generated texts by 10 pp in f1. further, (2) the texts with appraisal variables are longer and contain more details. this exemplifies the greater control for users.","wed, 26 jul 2023 07:34:19 utc (63 kb)"
"62","rare events of host switching for diseases using a sir model with mutations","yannick feld, alexander k. hartmann","physics and society (physics.soc-ph)","we numerically study disease dynamics that lead to the disease switching from one host species to another, resulting in diseases gaining the ability to infect, e.g., humans. unlike previous studies that focused on branching processes starting with the first infected humans, we begin by considering a disease pathogen that initially cannot infect humans. we model the entire process, starting from an infection in the animal population, including mutations that eventually enable the disease to cause an epidemic outbreak in the human population. we use an sir model on a network consisting of 132 dog and 1320 human nodes, with a single parameter representing the gene of the pathogen. we use numerical large-deviation techniques, specifically the $1/t$ wang-landau algorithm, to calculate the potentially very small probability of the host switching event. with this approach we are able to resolve probabilities as small as $10^{-120}$. additionally the $1/t$ wang-landau algorithm allows us to obtain the complete probability density function $p(c)$ of the cumulative fraction $c$ of infected humans, which is an indicator for the severity of the disease in the human population. we also calculate correlations of $c$ with selected quantities $q$ that characterize the outbreak. due to the application of the rare-event algorithm, this is possible for the entire range of $c$ values.","tue, 25 jul 2023 15:03:55 utc (8,015 kb)"
"63","using simulation to calibrate real data acquisition in veterinary medicine","krystian strzałka, szymon mazurek, maciej wielgosz, paweł russek, jakub caputa, daria łukasik, jan krupiński, jakub grzeszczyk, michał karwatowski, rafał frączek, ernest jamro, marcin pietroń, sebastian koryciak, agnieszka dąbrowska-boruch, kazimierz wiatr","machine learning (cs.lg)","this paper explores the innovative use of simulation environments to enhance data acquisition and diagnostics in veterinary medicine, focusing specifically on gait analysis in dogs. the study harnesses the power of blender and the blenderproc library to generate synthetic datasets that reflect diverse anatomical, environmental, and behavioral conditions. the generated data, represented in graph form and standardized for optimal analysis, is utilized to train machine learning algorithms for identifying normal and abnormal gaits. two distinct datasets with varying degrees of camera angle granularity are created to further investigate the influence of camera perspective on model accuracy. preliminary results suggest that this simulation-based approach holds promise for advancing veterinary diagnostics by enabling more precise data acquisition and more effective machine learning models. by integrating synthetic and real-world patient data, the study lays a robust foundation for improving overall effectiveness and efficiency in veterinary medicine.","fri, 21 jul 2023 16:50:10 utc (6,213 kb)"
"64","divide & bind your attention for improved generative semantic nursing","yumeng li, margret keuper, dan zhang, anna khoreva","computer vision and pattern recognition (cs.cv)","emerging large-scale text-to-image generative models, e.g., stable diffusion (sd), have exhibited overwhelming results with high fidelity. despite the magnificent progress, current state-of-the-art models still struggle to generate images fully adhering to the input prompt. prior work, attend & excite, has introduced the concept of generative semantic nursing (gsn), aiming to optimize cross-attention during inference time to better incorporate the semantics. it demonstrates promising results in generating simple prompts, e.g., ``a cat and a dog''. however, its efficacy declines when dealing with more complex prompts, and it does not explicitly address the problem of improper attribute binding. to address the challenges posed by complex prompts or scenarios involving multiple entities and to achieve improved attribute binding, we propose divide & bind. we introduce two novel loss objectives for gsn: a novel attendance loss and a binding loss. our approach stands out in its ability to faithfully synthesize desired objects with improved attribute alignment from complex prompts and exhibits superior performance across multiple evaluation benchmarks.","thu, 20 jul 2023 13:33:28 utc (48,477 kb)[v2] mon, 9 oct 2023 07:20:00 utc (48,476 kb)"
"65","important clues that facilitate visual emergence: three psychological experiments","jingmeng li, hui wei","neurons and cognition (q-bio.nc)","visual emergence is the phenomenon in which the visual system obtains a holistic perception after grouping and reorganizing local signals. the picture dalmatian dog is known for its use in explaining visual emergence. this type of image, which consists of a set of discrete black speckles (speckles), is called an emerging image. not everyone can find the dog in dalmatian dog, and among those who can, the time spent varies greatly. although gestalt theory summarizes perceptual organization into several principles, it remains ambiguous how these principles affect the perception of emerging images. this study, therefore, designed three psychological experiments to explore the factors that influence the perception of emerging images. in the first, we found that the density of speckles in the local area and the arrangements of some key speckles played a key role in the perception of an emerging case. we set parameters in the algorithm to characterize these two factors. we then automatically generated diversified emerging-test images (etis) through the algorithm and verified their effectiveness in two subsequent experiments.","mon, 10 jul 2023 13:46:43 utc (3,650 kb)"
"66","an overdensity of lyman break galaxies around the hot dust-obscured galaxy wise j224607.56$-$052634.9","dejene zewdie (udp), roberto j. assef, chiara mazzucchelli, manuel aravena, andrew w. blain, tanio díaz-santos, peter r. m. eisenhardt, hyunsung d. jun, daniel stern, chao-wei tsai, ""and"" jingwen w. wu","astrophysics of galaxies (astro-ph.ga)","we report the identification of lyman break galaxy (lbg) candidates around the most luminous hot dust-obscured galaxy (hot dog) known, wise j224607.56$-$052634.9 (w2246$-$0526) at $z=4.601$, using deep \textit{r}-, \textit{i}-, and \textit{z}-band imaging from the gemini multi-object spectrograph south (gmos-s). we use the surface density of lbgs to probe the mpc-scale environment of w2246$-$0526 to characterize its richness and evolutionary state. we identify lbg candidates in the vicinity of w2246$-$0526 using the selection criteria developed by \cite{2004vouchi} and \cite{2006yoshida} in the subaru deep field and in the subaru xmm-newton deep field, slightly modified to account for the difference between the filters used, and we find 37 and 55 lbg candidates, respectively. matching to the $z$-band depths of those studies, this corresponds to $\delta = 5.8^{+2.4}_{-1.9}$ times the surface density of lbgs expected in the field. interestingly, the hot dog itself, as well as a confirmed neighbor, do not satisfy either lbg selection criteria, suggesting we may be missing a large number of companion galaxies. our analysis shows that we are most likely only finding those with higher-than-average igm optical depth or moderately high dust obscuration. the number density of lbg candidates is not concentrated around w2246$-$0526, suggesting either an early evolutionary stage for the proto-cluster or that the hot dog may not be the most massive galaxy, or that the hot dog may be affecting the igm transparency in its vicinity. the overdensity around w2246$-$0526 is comparable to overdensities found around other hot dogs and is somewhat higher than typically found for radio galaxies and luminous quasars at a similar redshift.","thu, 29 jun 2023 17:59:11 utc (3,723 kb)"
"67","transforming a quadruped into a guide robot for the visually impaired: formalizing wayfinding, interaction modeling, and safety mechanism","j. taery kim, wenhao yu, yash kothari, jie tan, greg turk, sehoon ha","robotics (cs.ro)","this paper explores the principles for transforming a quadrupedal robot into a guide robot for individuals with visual impairments. a guide robot has great potential to resolve the limited availability of guide animals that are accessible to only two to three percent of the potential blind or visually impaired (bvi) users. to build a successful guide robot, our paper explores three key topics: (1) formalizing the navigation mechanism of a guide dog and a human, (2) developing a data-driven model of their interaction, and (3) improving user safety. first, we formalize the wayfinding task of the human-guide robot team using markov decision processes based on the literature and interviews. then we collect real human-robot interaction data from three visually impaired and six sighted people and develop an interaction model called the ``delayed harness'' to effectively simulate the navigation behaviors of the team. additionally, we introduce an action shielding mechanism to enhance user safety by predicting and filtering out dangerous actions. we evaluate the developed interaction model and the safety mechanism in simulation, which greatly reduce the prediction errors and the number of collisions, respectively. we also demonstrate the integrated system on a quadrupedal robot with a rigid harness, by guiding users over $100+$~m trajectories.","sat, 24 jun 2023 20:47:36 utc (35,082 kb)"
"68","language-guided generation of physically realistic robot motion and control","shusheng xu, huaijie wang, jiaxuan gao, yutao ouyang, chao yu, yi wu","robotics (cs.ro)","we aim to control a robot to physically behave in the real world following any high-level language command like ""cartwheel"" or ""kick. "" although human motion datasets exist, this task remains particularly challenging since generative models can produce physically unrealistic motions, which will be more severe for robots due to different body structures and physical properties. in addition, to control a physical robot to perform a desired motion, a control policy must be learned. we develop language-guided motion control (lagoon), a multi-phase method to generate physically realistic robot motions under language commands. lagoon first leverages a pre-trained model to generate human motion from a language command. then an rl phase is adopted to train a control policy in simulation to mimic the generated human motion. finally, with domain randomization, we show that our learned policy can be successfully deployed to a quadrupedal robot, leading to a robot dog that can stand up and wave its front legs in the real world to mimic the behavior of a hand-waving human.","sun, 18 jun 2023 10:28:48 utc (22,021 kb)"
"69","meta-personalizing vision-language models to find named instances in video","chun-hsiao yeh, bryan russell, josef sivic, fabian caba heilbron, simon jenni","computer vision and pattern recognition (cs.cv)","large-scale vision-language models (vlm) have shown impressive results for language-guided search applications. while these models allow category-level queries, they currently struggle with personalized searches for moments in a video where a specific object instance such as ``my dog biscuit'' appears. we present the following three contributions to address this problem. first, we describe a method to meta-personalize a pre-trained vlm, i.e., learning how to learn to personalize a vlm at test time to search in video. our method extends the vlm's token vocabulary by learning novel word embeddings specific to each instance. to capture only instance-specific features, we represent each instance embedding as a combination of shared and learned global category features. second, we propose to learn such personalization without explicit human supervision. our approach automatically identifies moments of named visual instances in video using transcripts and vision-language similarity in the vlm's embedding space. finally, we introduce this-is-my, a personal video instance retrieval benchmark. we evaluate our approach on this-is-my and deepfashion2 and show that we obtain a 15% relative improvement over the state of the art on the latter dataset.","fri, 16 jun 2023 20:12:11 utc (5,390 kb)"
"70","simplified temporal consistency reinforcement learning","yi zhao, wenshuai zhao, rinu boney, juho kannala, joni pajarinen","machine learning (cs.lg)","reinforcement learning is able to solve complex sequential decision-making tasks but is currently limited by sample efficiency and required computation. to improve sample efficiency, recent work focuses on model-based rl which interleaves model learning with planning. recent methods further utilize policy learning, value estimation, and, self-supervised learning as auxiliary objectives. in this paper we show that, surprisingly, a simple representation learning approach relying only on a latent dynamics model trained by latent temporal consistency is sufficient for high-performance rl. this applies when using pure planning with a dynamics model conditioned on the representation, but, also when utilizing the representation as policy and value function features in model-free rl. in experiments, our approach learns an accurate dynamics model to solve challenging high-dimensional locomotion tasks with online planners while being 4.1 times faster to train compared to ensemble-based methods. with model-free rl without planning, especially on high-dimensional tasks, such as the deepmind control suite humanoid and dog tasks, our approach outperforms model-free methods by a large margin and matches model-based methods' sample efficiency while training 2.4 times faster.","thu, 15 jun 2023 19:37:43 utc (1,112 kb)"
"71","""are you telling me to put glasses on the dog?'' content-grounded annotation of instruction clarification requests in the codraw dataset","brielen madureira, david schlangen","computation and language (cs.cl)","instruction clarification requests are a mechanism to solve communication problems, which is very functional in instruction-following interactions. recent work has argued that the codraw dataset is a valuable source of naturally occurring icrs. beyond identifying when icrs should be made, dialogue models should also be able to generate them with suitable form and content. in this work, we introduce codraw-icr (v2), extending the existing icr identifiers with fine-grained information grounded in the underlying dialogue game items and possible actions. our annotation can serve to model and evaluate repair capabilities of dialogue agents.","sun, 4 jun 2023 15:23:16 utc (8,111 kb)[v2] wed, 26 jul 2023 08:54:03 utc (8,112 kb)"
"72","barkour: benchmarking animal-level agility with quadruped robots","ken caluwaerts, atil iscen, j. chase kew, wenhao yu, tingnan zhang, daniel freeman, kuang-huei lee, lisa lee, stefano saliceti, vincent zhuang, nathan batchelor, steven bohez, federico casarini, jose enrique chen, omar cortes, erwin coumans, adil dostmohamed, gabriel dulac-arnold, alejandro escontrela, erik frey, roland hafner, deepali jain, bauyrjan jyenis, yuheng kuang, edward lee, linda luu, ofir nachum, ken oslund, jason powell, diego reyes, francesco romano, feresteh sadeghi, ron sloat, baruch tabanpour, daniel zheng, michael neunert, raia hadsell, nicolas heess, francesco nori, jeff seto, carolina parada, vikas sindhwani, vincent vanhoucke, jie tan","robotics (cs.ro)","animals have evolved various agile locomotion strategies, such as sprinting, leaping, and jumping. there is a growing interest in developing legged robots that move like their biological counterparts and show various agile skills to navigate complex environments quickly. despite the interest, the field lacks systematic benchmarks to measure the performance of control policies and hardware in agility. we introduce the barkour benchmark, an obstacle course to quantify agility for legged robots. inspired by dog agility competitions, it consists of diverse obstacles and a time based scoring mechanism. this encourages researchers to develop controllers that not only move fast, but do so in a controllable and versatile way. to set strong baselines, we present two methods for tackling the benchmark. in the first approach, we train specialist locomotion skills using on-policy reinforcement learning methods and combine them with a high-level navigation controller. in the second approach, we distill the specialist skills into a transformer-based generalist locomotion policy, named locomotion-transformer, that can handle various terrains and adjust the robot's gait based on the perceived environment and robot states. using a custom-built quadruped robot, we demonstrate that our method can complete the course at half the speed of a dog. we hope that our work represents a step towards creating controllers that enable robots to reach animal-level agility.","wed, 24 may 2023 02:49:43 utc (10,159 kb)"
"73","discovery of a low-redshift hot dust-obscured galaxy","guodong li, chao-wei tsai, daniel stern, jingwen wu, roberto j. assef, andrew w. blain, tanio díaz-santos, peter r. m. eisenhardt, roger l. griffith, thomas h. jarrett, hyunsung d. jun, sean e. lake, m. lynne saade","astrophysics of galaxies (astro-ph.ga)","we report the discovery of the hyperluminous, highly obscured agn wise j190445.04+485308.9 (w1904+4853 hereafter, $l_{bol} = 1.1 \times 10^{13} \ l_{\odot}$) at z=0.415. its well-sampled spectral energy distribution (sed) is dominated by infrared dust emission, though broad emission lines are detected in the optical spectra. these features suggest that w1904+4853 contains an actively accreting supermassive black hole hidden in its dusty cocoon, resembling the observed properties of hot dust-obscured galaxies (hot dogs), a population previously only identified at z>1.0. using the broad component of the mgii emission line, we estimate a black hole mass of $log \ (m_{bh}/m_{\odot}) = 8.4 \pm 0.4$. the corresponding eddington ratio of 1.4 implies that the central black hole accretion is at the theoretical limit of isotropic accretion. the rest-frame uv-optical sed also indicates that the host galaxy of w1904+4853 harbors strong star formation activity at the rate of $6-84 \ m_{\odot} \ \rm{yr^{-1}}$ with an independent estimate of sfr up to $\sim 45 \ m_{\odot} \ \rm{yr^{-1}}$ using the [o ii] emission line. with an estimated stellar mass of $3 \times 10^{10} \ m_{\odot}$, the host galaxy appears to be a starburst system with respect to the main sequence of the star-forming galaxies at the same redshift. although blueshifted and asymmetric [o iii] emission provides evidence of an outflow, we estimate it to be an order of magnitude smaller than the star formation rate, indicating that the current obscured agn activity at the center has not yet produced significant feedback on the host galaxy star formation activity. w1904+4853 supports the interpretation that hot dogs are a rare transitional phase of agn accretion in galaxy evolution, a phase that can persist into the present-day universe.","tue, 23 may 2023 06:46:09 utc (555 kb)[v2] thu, 6 jul 2023 19:08:10 utc (3,697 kb)[v3] tue, 21 nov 2023 21:32:11 utc (2,566 kb)"
"74","susana distancia is all you need: enforcing class separability in metric learning via two novel distance-based loss functions for few-shot image classification","mauricio mendez-ruiz, jorge gonzalez-zapata, ivan reyes-amezcua, daniel flores-araiza, francisco lopez-tiro, andres mendez-vazquez, gilberto ochoa-ruiz","computer vision and pattern recognition (cs.cv)","few-shot learning is a challenging area of research that aims to learn new concepts with only a few labeled samples of data. recent works based on metric-learning approaches leverage the meta-learning approach, which is encompassed by episodic tasks that make use a support (training) and query set (test) with the objective of learning a similarity comparison metric between those sets. due to the lack of data, the learning process of the embedding network becomes an important part of the few-shot task. previous works have addressed this problem using metric learning approaches, but the properties of the underlying latent space and the separability of the difference classes on it was not entirely enforced. in this work, we propose two different loss functions which consider the importance of the embedding vectors by looking at the intra-class and inter-class distance between the few data. the first loss function is the proto-triplet loss, which is based on the original triplet loss with the modifications needed to better work on few-shot scenarios. the second loss function, which we dub icnn loss is based on an inter and intra class nearest neighbors score, which help us to assess the quality of embeddings obtained from the trained network. our results, obtained from a extensive experimental setup show a significant improvement in accuracy in the miniimagennet benchmark compared to other metric-based few-shot learning methods by a margin of 2%, demonstrating the capability of these loss functions to allow the network to generalize better to previously unseen classes. in our experiments, we demonstrate competitive generalization capabilities to other domains, such as the caltech cub, dogs and cars datasets compared with the state of the art.","mon, 15 may 2023 23:12:09 utc (10,028 kb)[v2] wed, 17 may 2023 00:58:41 utc (10,028 kb)[v3] thu, 18 may 2023 20:41:34 utc (10,028 kb)"
"75","reconstructing animatable categories from videos","gengshan yang, chaoyang wang, n dinesh reddy, deva ramanan","computer vision and pattern recognition (cs.cv)","building animatable 3d models is challenging due to the need for 3d scans, laborious registration, and manual rigging, which are difficult to scale to arbitrary categories. recently, differentiable rendering provides a pathway to obtain high-quality 3d models from monocular videos, but these are limited to rigid categories or single instances. we present rac that builds category 3d models from monocular videos while disentangling variations over instances and motion over time. three key ideas are introduced to solve this problem: (1) specializing a skeleton to instances via optimization, (2) a method for latent space regularization that encourages shared structure across a category while maintaining instance details, and (3) using 3d background models to disentangle objects from the background. we show that 3d models of humans, cats, and dogs can be learned from 50-100 internet videos.","wed, 10 may 2023 17:56:21 utc (90,604 kb)"
"76","quantifying uncertainties on the tip of the red giant branch method","barry f. madore, wendy l. freedman kayla a. owens, in sung jang","solar and stellar astrophysics (astro-ph.sr)","we present an extensive grid of numerical simulations quantifying the uncertainties in measurements of the tip of the red giant branch (trgb). these simulations incorporate a luminosity function composed of 2 magnitudes of red giant branch (rgb) stars leading up to the tip, with asymptotic giant branch (agb) stars contributing exclusively to the luminosity function for at least a magnitude above the rgb tip. we quantify the sensitivity of the trgb detection and measurement to three important error sources: (1) the sample size of stars near the tip, (2) the photometric measurement uncertainties at the tip, and (3) the degree of self-crowding of the rgb population. the self-crowding creates a population of supra-trgb stars due to the blending of one or more rgb stars just below the tip. this last population is ultimately difficult, though still possible, to disentangle from true agb stars. in the analysis given here, the precepts and general methodology as used in the chicago-carnegie hubble program (cchp) has been followed. however, in the appendix, we introduce and test a set of new tip detection kernels which internally incorporate self-consistent smoothing. these are generalizations of the two-step model used by the cchp (smoothing followed by sobel-filter tip detection), where the new kernels are based on successive binomial-coefficient approximations to the derivative-of-a-gaussian (dog) edge detector, as is commonly used in modern digital image processing.","wed, 10 may 2023 14:27:03 utc (27,463 kb)"
"77","personalize segment anything model with one shot","renrui zhang, zhengkai jiang, ziyu guo, shilin yan, junting pan, xianzheng ma, hao dong, peng gao, hongsheng li","computer vision and pattern recognition (cs.cv)","driven by large-data pre-training, segment anything model (sam) has been demonstrated as a powerful and promptable framework, revolutionizing the segmentation models. despite the generality, customizing sam for specific visual concepts without man-powered prompting is under explored, e.g., automatically segmenting your pet dog in different images. in this paper, we propose a training-free personalization approach for sam, termed as persam. given only a single image with a reference mask, persam first localizes the target concept by a location prior, and segments it within other images or videos via three techniques: target-guided attention, target-semantic prompting, and cascaded post-refinement. in this way, we effectively adapt sam for private use without any training. to further alleviate the mask ambiguity, we present an efficient one-shot fine-tuning variant, persam-f. freezing the entire sam, we introduce two learnable weights for multi-scale masks, only training 2 parameters within 10 seconds for improved performance. to demonstrate our efficacy, we construct a new segmentation dataset, perseg, for personalized evaluation, and test our methods on video object segmentation with competitive performance. besides, our approach can also enhance dreambooth to personalize stable diffusion for text-to-image generation, which discards the background disturbance for better target appearance learning. code is released at this https url","thu, 4 may 2023 17:59:36 utc (39,764 kb)[v2] wed, 4 oct 2023 01:15:21 utc (32,396 kb)"
"78","lostpaw: finding lost pets using a contrastive learning-based transformer with visual input","andrei voinea, robin kock, maruf a. dhali","computer vision and pattern recognition (cs.cv)","losing pets can be highly distressing for pet owners, and finding a lost pet is often challenging and time-consuming. an artificial intelligence-based application can significantly improve the speed and accuracy of finding lost pets. in order to facilitate such an application, this study introduces a contrastive neural network model capable of accurately distinguishing between images of pets. the model was trained on a large dataset of dog images and evaluated through 3-fold cross-validation. following 350 epochs of training, the model achieved a test accuracy of 90%. furthermore, overfitting was avoided, as the test accuracy closely matched the training accuracy. our findings suggest that contrastive neural network models hold promise as a tool for locating lost pets. this paper provides the foundation for a potential web application that allows users to upload images of their missing pets, receiving notifications when matching images are found in the application's image database. this would enable pet owners to quickly and accurately locate lost pets and reunite them with their families.","fri, 28 apr 2023 11:23:44 utc (4,889 kb)"
"79","slomo: a general system for legged robot motion imitation from casual videos","john z. zhang, shuo yang, gengshan yang, arun l. bishop, deva ramanan, zachary manchester","robotics (cs.ro)","we present slomo: a first-of-its-kind framework for transferring skilled motions from casually captured ""in the wild"" video footage of humans and animals to legged robots. slomo works in three stages: 1) synthesize a physically plausible reconstructed key-point trajectory from monocular videos; 2) optimize a dynamically feasible reference trajectory for the robot offline that includes body and foot motion, as well as contact sequences that closely tracks the key points; 3) track the reference trajectory online using a general-purpose model-predictive controller on robot hardware. traditional motion imitation for legged motor skills often requires expert animators, collaborative demonstrations, and/or expensive motion capture equipment, all of which limits scalability. instead, slomo only relies on easy-to-obtain monocular video footage, readily available in online repositories such as youtube. it converts videos into motion primitives that can be executed reliably by real-world robots. we demonstrate our approach by transferring the motions of cats, dogs, and humans to example robots including a quadruped (on hardware) and a humanoid (in simulation). to the best knowledge of the authors, this is the first attempt at a general-purpose motion transfer framework that imitates animal and human motions on legged robots directly from casual videos without artificial markers or labels.","thu, 27 apr 2023 17:53:27 utc (42,614 kb)[v2] tue, 5 sep 2023 13:45:16 utc (7,802 kb)"
"80","cluster flow: how a hierarchical clustering layer make allows deep-nns more resilient to hacking, more human-like and easily implements relational reasoning","ella gale, oliver matthews","machine learning (cs.lg)","despite the huge recent breakthroughs in neural networks (nns) for artificial intelligence (specifically deep convolutional networks) such nns do not achieve human-level performance: they can be hacked by images that would fool no human and lack `common sense'. it has been argued that a basis of human-level intelligence is mankind's ability to perform relational reasoning: the comparison of different objects, measuring similarity, grasping of relations between objects and the converse, figuring out the odd one out in a set of objects. mankind can even do this with objects they have never seen before. here we show how clusterflow, a semi-supervised hierarchical clustering framework can operate on trained nns utilising the rich multi-dimensional class and feature data found at the pre-softmax layer to build a hyperspacial map of classes/features and this adds more human-like functionality to modern deep convolutional neural networks. we demonstrate this with 3 tasks. 1. the statistical learning based `mistakes' made by infants when attending to images of cats and dogs. 2. improving both the resilience to hacking images and the accurate measure of certainty in deep-nns. 3. relational reasoning over sets of images, including those not known to the nn nor seen before. we also demonstrate that clusterflow can work on non-nn data and deal with missing data by testing it on a chemistry dataset. this work suggests that modern deep nns can be made more human-like without re-training of the nns. as it is known that some methods used in deep and convolutional nns are not biologically plausible or perhaps even the best approach: the clusterflow framework can sit on top of any nn and will be a useful tool to add as nns are improved in this regard.","thu, 27 apr 2023 10:41:03 utc (6,003 kb)"
"81","hyperbolic image-text representations","karan desai, maximilian nickel, tanmay rajpurohit, justin johnson, ramakrishna vedantam","computer vision and pattern recognition (cs.cv)","visual and linguistic concepts naturally organize themselves in a hierarchy, where a textual concept ""dog"" entails all images that contain dogs. despite being intuitive, current large-scale vision and language models such as clip do not explicitly capture such hierarchy. we propose meru, a contrastive model that yields hyperbolic representations of images and text. hyperbolic spaces have suitable geometric properties to embed tree-like data, so meru can better capture the underlying hierarchy in image-text datasets. our results show that meru learns a highly interpretable and structured representation space while being competitive with clip's performance on standard multi-modal tasks like image classification and image-text retrieval. our code and models are available at this https url","tue, 18 apr 2023 17:59:45 utc (5,474 kb)[v2] tue, 6 jun 2023 00:33:42 utc (4,887 kb)[v3] thu, 18 jan 2024 17:13:21 utc (4,903 kb)"
"82","generating adversarial attacks in the latent space","nitish shukla, sudipta banerjee","machine learning (cs.lg)","adversarial attacks in the input (pixel) space typically incorporate noise margins such as $l_1$ or $l_{\infty}$-norm to produce imperceptibly perturbed data that confound deep learning networks. such noise margins confine the magnitude of permissible noise. in this work, we propose injecting adversarial perturbations in the latent (feature) space using a generative adversarial network, removing the need for margin-based priors. experiments on mnist, cifar10, fashion-mnist, cifar100 and stanford dogs datasets support the effectiveness of the proposed method in generating adversarial attacks in the latent space while ensuring a high degree of visual realism with respect to pixel-based adversarial attack methods.","mon, 10 apr 2023 05:11:59 utc (3,101 kb)"
"83","an atlas of the heterogeneous viscoelastic brain with local power-law attenuation synthesised using prony-series","oisin morrison, michel destrade, bharat b. tripathi","medical physics (physics.med-ph)","this review addresses the acute need to acknowledge the mechanical heterogeneity of brain matter and to accurately calibrate its local viscoelastic material properties accordingly. specifically, it is important to compile the existing and disparate literature on attenuation power laws and dispersion to make progress in wave physics of brain matter, a field of research that has the potential to explain the mechanisms at play in diffuse axonal injury and mild traumatic brain injury in general. currently, viscous effects in the brain are modelled using prony-series, i.e., a sum of decaying exponentials at different relaxation times. here we collect and synthesise the prony-series coefficients appearing in the literature for twelve regions: brainstem, basal ganglia, cerebellum, corona radiata, corpus callosum, cortex, dentate gyrus, hippocampus, thalamus, grey matter, white matter, homogeneous brain, and for eight different mammals: pig, rat, human, mouse, cow, sheep, monkey and dog. using this data, we compute the fractional-exponent attenuation power laws for different tissues of the brain, the corresponding dispersion laws resulting from causality, and the averaged prony-series coefficients.","wed, 5 apr 2023 17:30:21 utc (3,601 kb)"
"84","equivariant similarity for vision-language foundation models","tan wang, kevin lin, linjie li, chung-ching lin, zhengyuan yang, hanwang zhang, zicheng liu, lijuan wang","computer vision and pattern recognition (cs.cv)","this study explores the concept of equivariance in vision-language foundation models (vlms), focusing specifically on the multimodal similarity function that is not only the major training objective but also the core delivery to support downstream tasks. unlike the existing image-text similarity objective which only categorizes matched pairs as similar and unmatched pairs as dissimilar, equivariance also requires similarity to vary faithfully according to the semantic changes. this allows vlms to generalize better to nuanced and unseen multimodal compositions. however, modeling equivariance is challenging as the ground truth of semantic change is difficult to collect. for example, given an image-text pair about a dog, it is unclear to what extent the similarity changes when the pixel is changed from dog to cat? to this end, we propose eqsim, a regularization loss that can be efficiently calculated from any two matched training pairs and easily pluggable into existing image-text retrieval fine-tuning. meanwhile, to further diagnose the equivariance of vlms, we present a new challenging benchmark eqben. compared to the existing evaluation sets, eqben is the first to focus on ""visual-minimal change"". extensive experiments show the lack of equivariance in current vlms and validate the effectiveness of eqsim. code is available at this https url.","sat, 25 mar 2023 13:22:56 utc (3,726 kb)[v2] mon, 9 oct 2023 16:55:08 utc (3,991 kb)"
"85","searching a tree with signals: routing mobile sensors for targets emitting radiation, chemicals or scents","steve alpern, thomas lidbetter","discrete mathematics (cs.dm)","adversarial search of a network for an immobile hider (or target) was introduced and solved for rooted trees by gal (1979). in this zero-sum game, a hider picks a point to hide on the tree and a searcher picks a unit speed trajectory starting at the root. the payoff (to the hider) is the search time. in gal's model (and many subsequent investigations), the searcher receives no additional information after the hider chooses his location. in reality, the searcher will often receive such locational information. for homeland security, mobile sensors on vehicles have been used to locate radioactive material stashed in an urban environment. in a military setting, mobile sensors can detect chemical signatures from land mines. in predator-prey search, the predator often has specially attuned senses (hearing for wolves, vision for eagles, smell for dogs, sonar for bats, pressure sensors for sharks) that may help it locate the prey. how can such noisy locational information be used by the searcher to modify her route? we model such information as signals which indicate which of two branches of a binary tree should be searched first, where the signal has a known accuracy p<1. our solution calculates which branch (at every branch node) is favored, meaning it should always be searched first when the signal is in that direction. when the signal is in the other direction, we calculate the probability the signal should be followed. compared to the optimal hider strategy in the classic search game of gal, the hider's optimal distribution for this model is more skewed towards leaf nodes that are further from the root.","thu, 23 mar 2023 15:47:20 utc (69 kb)"
"86","semianalytically designed dual polarized printed-circuit-board (pcb) metagratings","yuval shklarsh, ariel epstein","applied physics (physics.app-ph)","metagratings (mgs), sparse (periodic) composites of subwavelength polarizable particles (meta-atoms), have demonstrated highly efficient diffraction engineering capabilities via meticulous tailoring of the interaction between individual scatterers. to date, mgs at microwave frequencies have mostly been devised for either transverse electric (te) or transverse magnetic (tm) polarized scenarios, which limits their use in many practical applications. herein, we bridge this gap and present a comprehensive semianalytical design method for dual-polarized mgs with a separable response for te and tm waves. first, by relating a printed circuit board (pcb) compatible array of dog-bone elements with the canonical dipole line analytical model, we establish a meta-atom for tm-polarized mgs, featuring negligible interaction with te waves. subsequently, we integrate the proposed configuration with a systematic synthesis scheme to implement a tm beam splitter mg, harnessing the equivalent dipole line model to resolve analytically the optimal meta-atom coordinates and dog-bone polarizability, without resorting to full-wave optimization. finally, we show that a dual-polarized mg beam splitter can be conveniently synthesized correspondingly, combining the tm-polarized structure as is with previously reported te-polarized mg designs. this work paves a clear path towards integration of sparse, semianlaytically synthesized, efficient mgs in practical dual-polarized communication and imaging applications.","tue, 21 mar 2023 21:21:51 utc (2,499 kb)"
"87","dynamic and polarimetric vlbi imaging with a multiscalar approach","hendrik müller, andrei lobanov","instrumentation and methods for astrophysics (astro-ph.im)","recently multiscale imaging approaches such as dog-hit were developed to solve the vlbi imaging problem and showed a promising performance: they are fast, accurate, unbiased and automatic. we extend the multiscalar imaging approach to polarimetric imaging, reconstructions of dynamically evolving sources and finally to dynamic polarimetric reconstructions. these extensions (mr-support imaging) utilize a multiscalar approach. the time-averaged stokes i image is decomposed by a wavelet transform into single subbands. we use the set of statistically significant wavelet coefficients, the multiresolution support, computed by dog-hit as a prior in a constrained minimization manner: we fit the single-frame (polarimetric) observables by only varying the coefficients in the multiresolution support. the eht is a vlbi array imaging supermassive black holes. we demonstrate on synthetic data that mr-support imaging offers ample regularization and is able to recover simple geometric dynamics at the horizon scale in a typical eht setup. the approach is relatively lightweight, fast and largely automatic and data driven. the ngeht is a planned extension of the eht designed to recover movies at the event horizon scales of a supermassive black hole. we benchmark the performance of mr-support imaging for the denser ngeht configuration demonstrating the major improvements the additional ngeht antennas will bring to dynamic, polarimetric reconstructions. current and upcoming instruments offer the observational possibility to do polarimetric imaging of dynamically evolving structural patterns with highest spatial and temporal resolution. state-of-the-art dynamic reconstruction methods can capture this motion with a range of temporal regularizers and priors. with this work, we add an additional, simpler regularizer to the list: constraining the reconstruction to the multiresolution support.","tue, 21 mar 2023 14:21:43 utc (9,285 kb)"
"88","legs as manipulator: pushing quadrupedal agility beyond locomotion","xuxin cheng, ashish kumar, deepak pathak","robotics (cs.ro)","locomotion has seen dramatic progress for walking or running across challenging terrains. however, robotic quadrupeds are still far behind their biological counterparts, such as dogs, which display a variety of agile skills and can use the legs beyond locomotion to perform several basic manipulation tasks like interacting with objects and climbing. in this paper, we take a step towards bridging this gap by training quadruped robots not only to walk but also to use the front legs to climb walls, press buttons, and perform object interaction in the real world. to handle this challenging optimization, we decouple the skill learning broadly into locomotion, which involves anything that involves movement whether via walking or climbing a wall, and manipulation, which involves using one leg to interact while balancing on the other three legs. these skills are trained in simulation using curriculum and transferred to the real world using our proposed sim2real variant that builds upon recent locomotion success. finally, we combine these skills into a robust long-term plan by learning a behavior tree that encodes a high-level task hierarchy from one clean expert demonstration. we evaluate our method in both simulation and real-world showing successful executions of both short as well as long-range tasks and how robustness helps confront external perturbations. videos at this https url","mon, 20 mar 2023 17:59:58 utc (4,518 kb)[v2] wed, 22 mar 2023 08:48:15 utc (4,518 kb)"
"89","escape: countering systematic errors from machine's blind spots via interactive visual analysis","yongsu ahn, yu-ru lin, panpan xu, zeng dai","machine learning (cs.lg)","classification models learn to generalize the associations between data samples and their target classes. however, researchers have increasingly observed that machine learning practice easily leads to systematic errors in ai applications, a phenomenon referred to as ai blindspots. such blindspots arise when a model is trained with training samples (e.g., cat/dog classification) where important patterns (e.g., black cats) are missing or periphery/undesirable patterns (e.g., dogs with grass background) are misleading towards a certain class. even more sophisticated techniques cannot guarantee to capture, reason about, and prevent the spurious associations. in this work, we propose escape, a visual analytic system that promotes a human-in-the-loop workflow for countering systematic errors. by allowing human users to easily inspect spurious associations, the system facilitates users to spontaneously recognize concepts associated misclassifications and evaluate mitigation strategies that can reduce biased associations. we also propose two statistical approaches, relative concept association to better quantify the associations between a concept and instances, and debias method to mitigate spurious associations. we demonstrate the utility of our proposed escape system and statistical measures through extensive evaluation including quantitative experiments, usage scenarios, expert interviews, and controlled user experiments.","thu, 16 mar 2023 21:29:50 utc (4,991 kb)"
"90","unified multi-modal latent diffusion for joint subject and text conditional image generation","yiyang ma, huan yang, wenjing wang, jianlong fu, jiaying liu","computer vision and pattern recognition (cs.cv)","language-guided image generation has achieved great success nowadays by using diffusion models. however, texts can be less detailed to describe highly-specific subjects such as a particular dog or a certain car, which makes pure text-to-image generation not accurate enough to satisfy user requirements. in this work, we present a novel unified multi-modal latent diffusion (umm-diffusion) which takes joint texts and images containing specified subjects as input sequences and generates customized images with the subjects. to be more specific, both input texts and images are encoded into one unified multi-modal latent space, in which the input images are learned to be projected to pseudo word embedding and can be further combined with text to guide image generation. besides, to eliminate the irrelevant parts of the input images such as background or illumination, we propose a novel sampling technique of diffusion models used by the image generator which fuses the results guided by multi-modal input and pure text input. by leveraging the large-scale pre-trained text-to-image generator and the designed image encoder, our method is able to generate high-quality images with complex semantics from both aspects of input texts and images.","thu, 16 mar 2023 13:50:20 utc (18,573 kb)"
"91","enable natural tactile interaction for robot dog based on large-format distributed flexible pressure sensors","lishuang zhan, yancheng cao, qitai chen, haole guo, jiasi gao, yiyue luo, shihui guo, guyue zhou, jiangtao gong","robotics (cs.ro)","touch is an important channel for human-robot interaction, while it is challenging for robots to recognize human touch accurately and make appropriate responses. in this paper, we design and implement a set of large-format distributed flexible pressure sensors on a robot dog to enable natural human-robot tactile interaction. through a heuristic study, we sorted out 81 tactile gestures commonly used when humans interact with real dogs and 44 dog reactions. a gesture classification algorithm based on resnet is proposed to recognize these 81 human gestures, and the classification accuracy reaches 98.7%. in addition, an action prediction algorithm based on transformer is proposed to predict dog actions from human gestures, reaching a 1-gram bleu score of 0.87. finally, we compare the tactile interaction with the voice interaction during a freedom human-robot-dog interactive playing study. the results show that tactile interaction plays a more significant role in alleviating user anxiety, stimulating user excitement and improving the acceptability of robot dogs.","tue, 14 mar 2023 02:35:04 utc (10,195 kb)"
"92","automatic detection of signalling behaviour from assistance dogs as they forecast the onset of epileptic seizures in humans","hitesh raju, ankit sharma, aoife smeaton, alan f. smeaton","machine learning (cs.lg)","epilepsy or the occurrence of epileptic seizures, is one of the world's most well-known neurological disorders affecting millions of people. seizures mostly occur due to non-coordinated electrical discharges in the human brain and may cause damage, including collapse and loss of consciousness. if the onset of a seizure can be forecast then the subject can be placed into a safe environment or position so that self-injury as a result of a collapse can be minimised. however there are no definitive methods to predict seizures in an everyday, uncontrolled environment. previous studies have shown that pet dogs have the ability to detect the onset of an epileptic seizure by scenting the characteristic volatile organic compounds exuded through the skin by a subject prior a seizure occurring and there are cases where assistance dogs, trained to scent the onset of a seizure, can signal this to their owner/trainer. in this work we identify how we can automatically detect the signalling behaviours of trained assistance dogs and use this to alert their owner. using data from an accelerometer worn on the collar of a dog we describe how we gathered movement data from 11 trained dogs for a total of 107 days as they exhibited signalling behaviour on command. we present the machine learning techniques used to accurately detect signalling from routine dog behaviour. this work is a step towards automatic alerting of the likely onset of an epileptic seizure from the signalling behaviour of a trained assistance dog.","sat, 11 mar 2023 13:32:02 utc (338 kb)"
"93","j1406+0102: dust obscured galaxy hiding super eddington accretion system with bright radio emission","hikaru fukuchi, kohei ichikawa, masayuki akiyama, shigeo kimura, yoshiki toba, kohei inayoshi, akatoki noboriguchi, toshihiro kawaguchi, xiaoyang chen, itsna k. fitriana","astrophysics of galaxies (astro-ph.ga)","recent high-$z$ quasar observations strongly indicate that super-eddington accretion is a crucial phase to describe the existence of supermassive black holes (smbhs) with $m_\mathrm{bh} \gtrsim 10^9 m_\odot$ at $z \gtrsim 7$. motivated by the theoretical suggestion that the super-eddington phase efficiently produces outflows and jets bright in radio bands, we search and find a super-eddington radio-loud dust-obscured galaxy (dog) j1406+0102 at $z=0.236$, through cross-matching of the infrared-bright dogs of noboriguchi et al. (2019) with the vla/first 1.4 ghz radio and the sdss optical spectral catalog. dog j1406+0102 shows broad components in the balmer lines. assuming those lines are from the broad line region, it gives bh mass estimation of $\log\ (m_\mathrm{bh}/m_\odot)=7.30 \pm 0.25$, and agn luminosity of $\log (l_\mathrm{bol,[oiii]}/\mathrm{erg}~\mathrm{s}^{-1}) = 45.91\pm0.38$ estimated from the intrinsic [oiii] luminosity, resulting in super-eddington accretion of $\lambda_\mathrm{edd}\simeq 3$. we show that 1) dog j1406+0102 is operating strong agn feedback: the [oiii] outflow velocity exceeds the escape velocity of the host galaxy halo and the kinetic efficiency is obtained as $\approx$ 8% that can be sufficient to quench the host galaxy, 2) the expected future growth pathway of dog j1406+0102 would join an over-massive bh trajectory and 3) radio-loud dogs can provide a significant contribution to the high-energy ($\gtrsim$ 100 tev) cosmic neutrino background if we assume dog j1406+0102 as a representative of radio-loud dogs.","thu, 9 mar 2023 22:16:22 utc (1,527 kb)"
"94","spawrious: a benchmark for fine control of spurious correlation biases","aengus lynch, gbètondji j-s dovonon, jean kaddour, ricardo silva","computer vision and pattern recognition (cs.cv)","the problem of spurious correlations (scs) arises when a classifier relies on non-predictive features that happen to be correlated with the labels in the training data. for example, a classifier may misclassify dog breeds based on the background of dog images. this happens when the backgrounds are correlated with other breeds in the training data, leading to misclassifications during test time. previous sc benchmark datasets suffer from varying issues, e.g., over-saturation or only containing one-to-one (o2o) scs, but no many-to-many (m2m) scs arising between groups of spurious attributes and classes. in this paper, we present \benchmark-\{o2o, m2m\}-\{easy, medium, hard\}, an image classification benchmark suite containing spurious correlations between classes and backgrounds. to create this dataset, we employ a text-to-image model to generate photo-realistic images and an image captioning model to filter out unsuitable ones. the resulting dataset is of high quality and contains approximately 152k images. our experimental results demonstrate that state-of-the-art group robustness methods struggle with \benchmark, most notably on the hard-splits with none of them getting over $70\%$ accuracy on the hardest split using a resnet50 pretrained on imagenet. by examining model misclassifications, we detect reliances on spurious backgrounds, demonstrating that our dataset provides a significant challenge.","thu, 9 mar 2023 18:22:12 utc (1,920 kb)[v2] thu, 8 jun 2023 18:00:46 utc (26,698 kb)[v3] mon, 12 jun 2023 14:04:53 utc (2,810 kb)"
"95","demystifying what code summarization models learned","yu wang, ke wang","programming languages (cs.pl)","study patterns that models have learned has long been a focus of pattern recognition research. explaining what patterns are discovered from training data, and how patterns are generalized to unseen data are instrumental to understanding and advancing the pattern recognition methods. unfortunately, the vast majority of the application domains deal with continuous data (i.e. statistical in nature) out of which extracted patterns can not be formally defined. for example, in image classification, there does not exist a principle definition for a label of cat or dog. even in natural language, the meaning of a word can vary with the context it is surrounded by. unlike the aforementioned data format, programs are a unique data structure with a well-defined syntax and semantics, which creates a golden opportunity to formalize what models have learned from source code. this paper presents the first formal definition of patterns discovered by code summarization models (i.e. models that predict the name of a method given its body), and gives a sound algorithm to infer a context-free grammar (cfg) that formally describes the learned patterns. we realize our approach in patic which produces cfgs for summarizing the patterns discovered by code summarization models. in particular, we pick two prominent instances, code2vec and code2seq, to evaluate patic. patic shows that the patterns extracted by each model are heavily restricted to local, and syntactic code structures with little to none semantic implication. based on these findings, we present two example uses of the formal definition of patterns: a new method for evaluating the robustness and a new technique for improving the accuracy of code summarization models. our work opens up this exciting, new direction of studying what models have learned from source code.","sat, 4 mar 2023 06:06:15 utc (5,696 kb)"
"96","3d generation on imagenet","ivan skorokhodov, aliaksandr siarohin, yinghao xu, jian ren, hsin-ying lee, peter wonka, sergey tulyakov","computer vision and pattern recognition (cs.cv)","existing 3d-from-2d generators are typically designed for well-curated single-category datasets, where all the objects have (approximately) the same scale, 3d location, and orientation, and the camera always points to the center of the scene. this makes them inapplicable to diverse, in-the-wild datasets of non-alignable scenes rendered from arbitrary camera poses. in this work, we develop a 3d generator with generic priors (3dgp): a 3d synthesis framework with more general assumptions about the training data, and show that it scales to very challenging datasets, like imagenet. our model is based on three new ideas. first, we incorporate an inaccurate off-the-shelf depth estimator into 3d gan training via a special depth adaptation module to handle the imprecision. then, we create a flexible camera model and a regularization strategy for it to learn its distribution parameters during training. finally, we extend the recent ideas of transferring knowledge from pre-trained classifiers into gans for patch-wise trained models by employing a simple distillation-based technique on top of the discriminator. it achieves more stable training than the existing methods and speeds up the convergence by at least 40%. we explore our model on four datasets: sdip dogs 256x256, sdip elephants 256x256, lsun horses 256x256, and imagenet 256x256, and demonstrate that 3dgp outperforms the recent state-of-the-art in terms of both texture and geometry quality. code and visualizations: this https url.","thu, 2 mar 2023 17:06:57 utc (24,236 kb)"
"97","teaching clip to count to ten","roni paiss, ariel ephrat, omer tov, shiran zada, inbar mosseri, michal irani, tali dekel","computer vision and pattern recognition (cs.cv)","large vision-language models (vlms), such as clip, learn rich joint image-text representations, facilitating advances in numerous downstream tasks, including zero-shot classification and text-to-image generation. nevertheless, existing vlms exhibit a prominent well-documented limitation - they fail to encapsulate compositional concepts such as counting. we introduce a simple yet effective method to improve the quantitative understanding of vlms, while maintaining their overall performance on common benchmarks. specifically, we propose a new counting-contrastive loss used to finetune a pre-trained vlm in tandem with its original objective. our counting loss is deployed over automatically-created counterfactual examples, each consisting of an image and a caption containing an incorrect object count. for example, an image depicting three dogs is paired with the caption ""six dogs playing in the yard"". our loss encourages discrimination between the correct caption and its counterfactual variant which serves as a hard negative example. to the best of our knowledge, this work is the first to extend clip's capabilities to object counting. furthermore, we introduce ""countbench"" - a new image-text counting benchmark for evaluating a model's understanding of object counting. we demonstrate a significant improvement over state-of-the-art baseline models on this task. finally, we leverage our count-aware clip model for image retrieval and text-conditioned image generation, demonstrating that our model can produce specific counts of objects more reliably than existing ones.","thu, 23 feb 2023 14:43:53 utc (21,440 kb)"
"98","dog is sgd's best friend: a parameter-free dynamic step size schedule","maor ivgi, oliver hinder, yair carmon","machine learning (cs.lg)","we propose a tuning-free dynamic sgd step size formula, which we call distance over gradients (dog). the dog step sizes depend on simple empirical quantities (distance from the initial point and norms of gradients) and have no ``learning rate'' parameter. theoretically, we show that a slight variation of the dog formula enjoys strong parameter-free convergence guarantees for stochastic convex optimization assuming only \emph{locally bounded} stochastic gradients. empirically, we consider a broad range of vision and language transfer learning tasks, and show that dog's performance is close to that of sgd with tuned learning rate. we also propose a per-layer variant of dog that generally outperforms tuned sgd, approaching the performance of tuned adam. a pytorch implementation is available at this https url","wed, 8 feb 2023 18:28:48 utc (492 kb)[v2] wed, 31 may 2023 14:51:25 utc (537 kb)[v3] sun, 16 jul 2023 21:12:43 utc (538 kb)"
"99","semantic feature integration network for fine-grained visual classification","hui wang, yueyang li, haichi luo","computer vision and pattern recognition (cs.cv)","fine-grained visual classification (fgvc) is known as a challenging task due to subtle differences among subordinate categories. many current fgvc approaches focus on identifying and locating discriminative regions by using the attention mechanism, but neglect the presence of unnecessary features that hinder the understanding of object structure. these unnecessary features, including 1) ambiguous parts resulting from the visual similarity in object appearances and 2) noninformative parts (e.g., background noise), can have a significant adverse impact on classification results. in this paper, we propose the semantic feature integration network (sfi-net) to address the above difficulties. by eliminating unnecessary features and reconstructing the semantic relations among discriminative features, our sfi-net has achieved satisfying performance. the network consists of two modules: 1) the multi-level feature filter (mff) module is proposed to remove unnecessary features with different receptive field, and then concatenate the preserved features on pixel level for subsequent disposal; 2) the semantic information reconstitution (sir) module is presented to further establish semantic relations among discriminative features obtained from the mff module. these two modules are carefully designed to be light-weighted and can be trained end-to-end in a weakly-supervised way. extensive experiments on four challenging fine-grained benchmarks demonstrate that our proposed sfi-net achieves the state-of-the-arts performance. especially, the classification accuracy of our model on cub-200-2011 and stanford dogs reaches 92.64% and 93.03%, respectively.","mon, 13 feb 2023 07:32:25 utc (2,307 kb)"
"100","boundary guided learning-free semantic control with diffusion models","ye zhu, yu wu, zhiwei deng, olga russakovsky, yan yan","computer vision and pattern recognition (cs.cv)","applying pre-trained generative denoising diffusion models (ddms) for downstream tasks such as image semantic editing usually requires either fine-tuning ddms or learning auxiliary editing networks in the existing literature. in this work, we present our boundarydiffusion method for efficient, effective and light-weight semantic control with frozen pre-trained ddms, without learning any extra networks. as one of the first learning-free diffusion editing works, we start by seeking a comprehensive understanding of the intermediate high-dimensional latent spaces by theoretically and empirically analyzing their probabilistic and geometric behaviors in the markov chain. we then propose to further explore the critical step for editing in the denoising trajectory that characterizes the convergence of a pre-trained ddm and introduce an automatic search method. last but not least, in contrast to the conventional understanding that ddms have relatively poor semantic behaviors, we prove that the critical latent space we found already exhibits semantic subspace boundaries at the generic level in unconditional ddms, which allows us to do controllable manipulation by guiding the denoising trajectory towards the targeted boundary via a single-step operation. we conduct extensive experiments on multiple dpms architectures (ddpm, iddpm) and datasets (celeba, celeba-hq, lsun-church, lsun-bedroom, afhq-dog) with different resolutions (64, 256), achieving superior or state-of-the-art performance in various task scenarios (image semantic editing, text-based editing, unconditional semantic control) to demonstrate the effectiveness.","thu, 16 feb 2023 15:21:46 utc (2,137 kb)[v2] fri, 13 oct 2023 18:22:46 utc (3,550 kb)[v3] wed, 18 oct 2023 16:35:49 utc (3,550 kb)"
"101","infonce loss provably learns cluster-preserving representations","advait parulekar, liam collins, karthikeyan shanmugam, aryan mokhtari, sanjay shakkottai","machine learning (cs.lg)","the goal of contrasting learning is to learn a representation that preserves underlying clusters by keeping samples with similar content, e.g. the ``dogness'' of a dog, close to each other in the space generated by the representation. a common and successful approach for tackling this unsupervised learning problem is minimizing the infonce loss associated with the training samples, where each sample is associated with their augmentations (positive samples such as rotation, crop) and a batch of negative samples (unrelated samples). to the best of our knowledge, it was unanswered if the representation learned by minimizing the infonce loss preserves the underlying data clusters, as it only promotes learning a representation that is faithful to augmentations, i.e., an image and its augmentations have the same representation. our main result is to show that the representation learned by infonce with a finite number of negative samples is also consistent with respect to clusters in the data, under the condition that the augmentation sets within clusters may be non-overlapping but are close and intertwined, relative to the complexity of the learning function class.","wed, 15 feb 2023 19:45:35 utc (6,207 kb)"
"102","socrates: text-based human search and approach using a robot dog","jeongeun park, jefferson silveria, matthew pan, sungjoon choi","robotics (cs.ro)","in this paper, we propose a socratic model for robots approaching humans based on text system (socrates) focusing on the human search and approach based on free-form textual description; the robot first searches for the target user, then the robot proceeds to approach in a human-friendly manner. in particular, textual descriptions are composed of appearance (e.g., wearing white shirts with black hair) and location clues (e.g., is a student who works with robots). we initially present a human search socratic model that connects large pre-trained models in the language domain to solve the downstream task, which is searching for the target person based on textual descriptions. then, we propose a hybrid learning-based framework for generating target-cordial robotic motion to approach a person, consisting of a learning-from-demonstration module and a knowledge distillation module. we validate the proposed searching module via simulation using a virtual mobile robot as well as through real-world experiments involving participants and the boston dynamics spot robot. furthermore, we analyze the properties of the proposed approaching framework with human participants based on the robotic social attributes scale (rosas)","fri, 10 feb 2023 15:35:24 utc (39,170 kb)[v2] sun, 18 jun 2023 07:30:54 utc (9,862 kb)"
"103","leveraging supplementary text data to kick-start automatic speech recognition system development with limited transcriptions","nay san, martijn bartelds, blaine billings, ella de falco, hendi feriza, johan safri, wawan sahrozi, ben foley, bradley mcdonnell, dan jurafsky","computation and language (cs.cl)","recent research using pre-trained transformer models suggests that just 10 minutes of transcribed speech may be enough to fine-tune such a model for automatic speech recognition (asr) -- at least if we can also leverage vast amounts of text data (803 million tokens). but is that much text data necessary? we study the use of different amounts of text data, both for creating a lexicon that constrains asr decoding to possible words (e.g. *dogz vs. dogs), and for training larger language models that bias the system toward probable word sequences (e.g. too dogs vs. two dogs). we perform experiments using 10 minutes of transcribed speech from english (for replicating prior work) and two additional pairs of languages differing in the availability of supplemental text data: gronings and frisian (~7.5m token corpora available), and besemah and nasal (only small lexica available). for all languages, we found that using only a lexicon did not appreciably improve asr performance. for gronings and frisian, we found that lexica and language models derived from 'novel-length' 80k token subcorpora reduced the word error rate (wer) to 39% on average. our findings suggest that where a text corpus in the upper tens of thousands of tokens or more is available, fine-tuning a transformer model with just tens of minutes of transcribed speech holds some promise towards obtaining human-correctable transcriptions near the 30% wer rule-of-thumb.","thu, 9 feb 2023 23:30:49 utc (7,011 kb)"
"104","neural congealing: aligning images to a joint semantic atlas","dolev ofri-amar, michal geyer, yoni kasten, tali dekel","computer vision and pattern recognition (cs.cv)","we present neural congealing -- a zero-shot self-supervised framework for detecting and jointly aligning semantically-common content across a given set of images. our approach harnesses the power of pre-trained dino-vit features to learn: (i) a joint semantic atlas -- a 2d grid that captures the mode of dino-vit features in the input set, and (ii) dense mappings from the unified atlas to each of the input images. we derive a new robust self-supervised framework that optimizes the atlas representation and mappings per image set, requiring only a few real-world images as input without any additional input information (e.g., segmentation masks). notably, we design our losses and training paradigm to account only for the shared content under severe variations in appearance, pose, background clutter or other distracting objects. we demonstrate results on a plethora of challenging image sets including sets of mixed domains (e.g., aligning images depicting sculpture and artwork of cats), sets depicting related yet different object categories (e.g., dogs and tigers), or domains for which large-scale training data is scarce (e.g., coffee mugs). we thoroughly evaluate our method and show that our test-time optimization approach performs favorably compared to a state-of-the-art method that requires extensive training on large-scale datasets.","wed, 8 feb 2023 09:26:22 utc (27,166 kb)[v2] mon, 6 mar 2023 18:12:46 utc (27,166 kb)"
"105","ganravel: user-driven direction disentanglement in generative adversarial networks","noyan evirgen, xiang 'anthony' chen","human-computer interaction (cs.hc)","generative adversarial networks (gans) have many application areas including image editing, domain translation, missing data imputation, and support for creative work. however, gans are considered 'black boxes'. specifically, the end-users have little control over how to improve editing directions through disentanglement. prior work focused on new gan architectures to disentangle editing directions. alternatively, we propose ganravel a user-driven direction disentanglement tool that complements the existing gan architectures and allows users to improve editing directions iteratively. in two user studies with 16 participants each, ganravel users were able to disentangle directions and outperformed the state-of-the-art direction discovery baselines in disentanglement performance. in the second user study, ganravel was used in a creative task of creating dog memes and was able to create high-quality edited images and gifs.","tue, 31 jan 2023 20:21:01 utc (16,782 kb)"
"106","quantified canine: inferring dog personality from wearables","lakmal meegahapola, marios constantinides, zoran radivojevic, hongwei li, daniele quercia, michael s. eggleston","human-computer interaction (cs.hc)","being able to assess dog personality can be used to, for example, match shelter dogs with future owners, and personalize dog activities. such an assessment typically relies on experts or psychological scales administered to dog owners, both of which are costly. to tackle that challenge, we built a device called ""patchkeeper"" that can be strapped on the pet's chest and measures activity through an accelerometer and a gyroscope. in an in-the-wild deployment involving 12 healthy dogs, we collected 1300 hours of sensor activity data and dog personality test results from two validated questionnaires. by matching these two datasets, we trained ten machine-learning classifiers that predicted dog personality from activity data, achieving aucs in [0.63-0.90], suggesting the value of tracking the psychological signals of pets using wearable technologies.","tue, 17 jan 2023 15:37:16 utc (4,820 kb)[v2] wed, 25 jan 2023 20:12:53 utc (4,820 kb)"
"107","numerical analysis of pit-to-crack transition under corrosion fatigue using a stochastic pit generation algorithm","mojtaba mokhtari, xintong wang, jorgen amdahl","numerical analysis (math.na)","corrosion fatigue is a major threat to the integrity of marine structures. however, the combined damaging effect of corrosion and fatigue is not well understood due to the uncertainties associated with the stochastic nature of pitting corrosion. a pitting corrosion defect could have a quite complex morphology that varies randomly from one case to another. nevertheless, in numerical corrosion fatigue studies, the complex pit morphology is often idealized using an overly simplified geometry with a smooth surface because of the diffi-culties involved in modelling defects with the irregular and random shapes of pitting corrosion. the present study investigates the influence of such geometrical simplifications on the results of numerical corrosion fatigue analyses. for this purpose, an isolated complex-shaped pit, generated using a hierarchical stochastic algorithm scripted in python and linked with abaqus/cae, is developed in a q235 steel dog-bone specimen. the numer-ical results obtained from this model are compared with those from another model containing an idealized counterpart of the irregular pit. a discussion on the effect of pit morphology on the stress/strain history and distribution and fatigue crack initiation is presented.","mon, 16 jan 2023 12:50:15 utc (1,562 kb)"
"108","multimodality helps unimodality: cross-modal few-shot learning with multimodal models","zhiqiu lin, samuel yu, zhiyi kuang, deepak pathak, deva ramanan","computer vision and pattern recognition (cs.cv)","the ability to quickly learn a new task with minimal instruction - known as few-shot learning - is a central aspect of intelligent agents. classical few-shot benchmarks make use of few-shot samples from a single modality, but such samples may not be sufficient to characterize an entire concept class. in contrast, humans use cross-modal information to learn new concepts efficiently. in this work, we demonstrate that one can indeed build a better ${\bf visual}$ dog classifier by ${\bf read}$ing about dogs and ${\bf listen}$ing to them bark. to do so, we exploit the fact that recent multimodal foundation models such as clip are inherently cross-modal, mapping different modalities to the same representation space. specifically, we propose a simple cross-modal adaptation approach that learns from few-shot examples spanning different modalities. by repurposing class names as additional one-shot training samples, we achieve sota results with an embarrassingly simple linear classifier for vision-language adaptation. furthermore, we show that our approach can benefit existing methods such as prefix tuning, adapters, and classifier ensembling. finally, to explore other modalities beyond vision and language, we construct the first (to our knowledge) audiovisual few-shot benchmark and use cross-modal training to improve the performance of both image and audio classification.","mon, 16 jan 2023 05:40:42 utc (8,030 kb)[v2] wed, 18 jan 2023 02:13:43 utc (8,030 kb)[v3] sun, 29 jan 2023 21:05:53 utc (21,827 kb)[v4] thu, 3 aug 2023 01:56:35 utc (38,848 kb)"
"109","learning support and trivial prototypes for interpretable image classification","chong wang, yuyuan liu, yuanhong chen, fengbei liu, yu tian, davis j. mccarthy, helen frazer, gustavo carneiro","computer vision and pattern recognition (cs.cv)","prototypical part network (protopnet) methods have been designed to achieve interpretable classification by associating predictions with a set of training prototypes, which we refer to as trivial prototypes because they are trained to lie far from the classification boundary in the feature space. note that it is possible to make an analogy between protopnet and support vector machine (svm) given that the classification from both methods relies on computing similarity with a set of training points (i.e., trivial prototypes in protopnet, and support vectors in svm). however, while trivial prototypes are located far from the classification boundary, support vectors are located close to this boundary, and we argue that this discrepancy with the well-established svm theory can result in protopnet models with inferior classification accuracy. in this paper, we aim to improve the classification of protopnet with a new method to learn support prototypes that lie near the classification boundary in the feature space, as suggested by the svm theory. in addition, we target the improvement of classification results with a new model, named st-protopnet, which exploits our support prototypes and the trivial prototypes to provide more effective classification. experimental results on cub-200-2011, stanford cars, and stanford dogs datasets demonstrate that st-protopnet achieves state-of-the-art classification accuracy and interpretability results. we also show that the proposed support prototypes tend to be better localised in the object of interest rather than in the background region.","sun, 8 jan 2023 09:27:41 utc (9,379 kb)[v2] mon, 13 mar 2023 06:37:31 utc (8,402 kb)[v3] mon, 21 aug 2023 12:30:12 utc (18,584 kb)[v4] sun, 22 oct 2023 14:20:27 utc (18,584 kb)"
"110","distributed multirobot control for non-cooperative herding","nishant mohanty, jaskaran grover, changliu liu, katia sycara","robotics (cs.ro)","in this paper, we consider the problem of protecting a high-value area from being breached by sheep agents by crafting motions for dog robots. we use control barrier functions to pose constraints on the dogs' velocities that induce repulsion in the sheep relative to the high-value area. this paper extends the results developed in our prior work on the same topic in three ways. firstly, we implement and validate our previously developed centralized herding algorithm on many robots. we show herding of up to five sheep agents using three dog robots. secondly, as an extension to the centralized approach, we develop two distributed herding algorithms, one favoring feasibility while the other favoring optimality. in the first algorithm, we allocate a unique sheep to a unique dog, making that dog responsible for herding its allocated sheep away from the protected zone. we provide feasibility proof for this approach, along with numerical simulations. in the second algorithm, we develop an iterative distributed reformulation of the centralized algorithm, which inherits the optimality (i.e. budget efficiency) from the centralized approach. lastly, we conduct real-world experiments of these distributed algorithms and demonstrate herding of up to five sheep agents using five dog robots.","mon, 9 jan 2023 12:15:53 utc (34,205 kb)[v2] sun, 5 mar 2023 15:55:01 utc (34,187 kb)"
"111","deep leakage from gradients","yaqiong mu","cryptography and security (cs.cr)","with the development of artificial intelligence technology, federated learning (fl) model has been widely used in many industries for its high efficiency and confidentiality. some researchers have explored its confidentiality and designed some algorithms to attack training data sets, but these algorithms all have their own limitations. therefore, most people still believe that local machine learning gradient information is safe and reliable. in this paper, an algorithm based on gradient features is designed to attack the federated learning model in order to attract more attention to the security of federated learning systems. in federated learning system, gradient contains little information compared with the original training data set, but this project intends to restore the original training image data through gradient information. convolutional neural network (cnn) has excellent performance in image processing. therefore, the federated learning model of this project is equipped with convolutional neural network structure, and the model is trained by using image data sets. the algorithm calculates the virtual gradient by generating virtual image labels. then the virtual gradient is matched with the real gradient to restore the original image. this attack algorithm is written in python language, uses cat and dog classification kaggle data sets, and gradually extends from the full connection layer to the convolution layer, thus improving the universality. at present, the average squared error between the data recovered by this algorithm and the original image information is approximately 5, and the vast majority of images can be completely restored according to the gradient information given, indicating that the gradient of federated learning system is not absolutely safe and reliable.","thu, 15 dec 2022 08:06:46 utc (737 kb)"
"112","exploring levels of control for a navigation assistant for blind travelers","vinitha ranganeni, mike sinclair, eyal ofek, amos miller, jonathan campbell, andrey kolobov, edward cutrell","robotics (cs.ro)","only a small percentage of blind and low-vision people use traditional mobility aids such as a cane or a guide dog. various assistive technologies have been proposed to address the limitations of traditional mobility aids. these devices often give either the user or the device majority of the control. in this work, we explore how varying levels of control affect the users' sense of agency, trust in the device, confidence, and successful navigation. we present glide, a novel mobility aid with two modes for control: glide-directed and user-directed. we employ glide in a study (n=9) in which blind or low-vision participants used both modes to navigate through an indoor environment. overall, participants found that glide was easy to use and learn. most participants trusted glide despite its current limitations, and their confidence and performance increased as they continued to use glide. users' control mode preference varied in different situations; no single mode ""won"" in all situations.","thu, 5 jan 2023 23:55:49 utc (18,023 kb)"
"113","e-inu: simulating a quadruped robot with emotional sentience","abhiruph chakravarty, jatin karthik tripathy, sibi chakkaravarthy s, aswani kumar cherukuri, s. anitha, firuz kamalov, annapurna jonnalagadda","robotics (cs.ro)","quadruped robots are currently used in industrial robotics as mechanical aid to automate several routine tasks. however, presently, the usage of such a robot in a domestic setting is still very much a part of the research. this paper discusses the understanding and virtual simulation of such a robot capable of detecting and understanding human emotions, generating its gait, and responding via sounds and expression on a screen. to this end, we use a combination of reinforcement learning and software engineering concepts to simulate a quadruped robot that can understand emotions, navigate through various terrains and detect sound sources, and respond to emotions using audio-visual feedback. this paper aims to establish the framework of simulating a quadruped robot that is emotionally intelligent and can primarily respond to audio-visual stimuli using motor or audio response. the emotion detection from the speech was not as performant as eranns or zeta policy learning, still managing an accuracy of 63.5%. the video emotion detection system produced results that are almost at par with the state of the art, with an accuracy of 99.66%. due to its ""on-policy"" learning process, the ppo algorithm was extremely rapid to learn, allowing the simulated dog to demonstrate a remarkably seamless gait across the different cadences and variations. this enabled the quadruped robot to respond to generated stimuli, allowing us to conclude that it functions as predicted and satisfies the aim of this work.","tue, 3 jan 2023 06:28:45 utc (863 kb)"
"114","foreground-background separation through concept distillation from generative image foundation models","mischa dombrowski, hadrien reynaud, matthew baugh, bernhard kainz","computer vision and pattern recognition (cs.cv)","curating datasets for object segmentation is a difficult task. with the advent of large-scale pre-trained generative models, conditional image generation has been given a significant boost in result quality and ease of use. in this paper, we present a novel method that enables the generation of general foreground-background segmentation models from simple textual descriptions, without requiring segmentation labels. we leverage and explore pre-trained latent diffusion models, to automatically generate weak segmentation masks for concepts and objects. the masks are then used to fine-tune the diffusion model on an inpainting task, which enables fine-grained removal of the object, while at the same time providing a synthetic foreground and background dataset. we demonstrate that using this method beats previous methods in both discriminative and generative performance and closes the gap with fully supervised training while requiring no pixel-wise object labels. we show results on the task of segmenting four different objects (humans, dogs, cars, birds) and a use case scenario in medical image analysis. the code is available at this https url.","thu, 29 dec 2022 13:51:54 utc (39,715 kb)[v2] mon, 4 sep 2023 07:44:31 utc (18,058 kb)"
"115","cats vs dogs, photons vs hadrons","francesco visconti","instrumentation and methods for astrophysics (astro-ph.im)","in gamma ray astronomy with cherenkov telescopes, machine learning models are needed to guess what kind of particles generated the detected light, and their energies and directions. the focus in this work is on the classification task, training a simple convolutional neural network suitable for binary classification (as it could be a cats vs dogs classification problem), using as input uncleaned images generated by montecarlo data for a single astri telescope. results show an enhanced discriminant power with respect to classical random forest methods.","fri, 16 dec 2022 09:50:23 utc (118 kb)"
"116","auxiliary learning as a step towards artificial general intelligence","christeen t. jose","artificial intelligence (cs.ai)","auxiliary learning is a machine learning approach in which the model acknowledges the existence of objects that do not come under any of its learned categories.the name auxiliary learning was chosen due to the introduction of an auxiliary class. the paper focuses on increasing the generality of existing narrow purpose neural networks and also highlights the need to handle unknown objects. the cat & dog binary classifier is taken as an example throughout the paper.","wed, 30 nov 2022 19:04:50 utc (47 kb)"
"117","extreme nature of four blue-excess dust-obscured galaxies revealed by optical spectroscopy","akatoki noboriguchi, tohru nagao, yoshiki toba, kohei ichikawa, masaru kajisawa, nanako kato, toshihiro kawaguchi, hideo matsuhara, yoshiki matsuoka, kyoko onishi, masafusa onoue, nozomu tamada, koki terao, yuichi terashima, yoshihiro ueda, takuji yamashita","astrophysics of galaxies (astro-ph.ga)","we report optical spectroscopic observations of four blue-excess dust-obscured galaxies (bludogs) identified by subaru hyper suprime-cam. bludogs are a sub-class of dust-obscured galaxies (dogs, defined with the extremely red color $(i-[22])_{\rm ab} \geq 7.0$; toba et al. 2015), showing a significant flux excess in the optical $g$- and $r$-bands over the power-law fits to the fluxes at the longer wavelengths. noboriguchi et al. (2019) has suggested that bludogs may correspond to the blowing-out phase involved in a gas-rich major merger scenario. however the detailed properties of bludogs are not understood because of the lack of spectroscopic information. in this work, we carry out deep optical spectroscopic observations of four bludogs using subaru/focas and vlt/fors2. the obtained spectra show broad emission lines with extremely large equivalent widths, and a blue wing in the civ line profile. the redshifts are between 2.2 and 3.3. the averaged rest-frame equivalent widths of the civ lines are $160\pm33$ $\mathrm{\mathring{a}}$, $\sim$7 times higher than the average of a typical type-1 quasar. the fwhms of their velocity profiles are between 1990 and 4470 ${\rm km\ s^{-1}}$, and their asymmetric parameters are 0.05 and 0.25. such strong civ lines significantly affect the broad-band magnitudes, which is partly the origin of the blue excess seen in the spectral energy distribution of bludogs. their estimated supermassive black hole masses are $1.1\times10^8 < m_{\rm bh}/m_\odot < 5.5 \times 10^8$. the inferred eddington ratios of the bludogs are higher than 1 ($1.1< \lambda_{\rm edd} < 3.8$), suggesting that the bludogs are in a rapidly evolving phase of supermassive black holes.","wed, 30 nov 2022 05:31:16 utc (1,525 kb)"
"118","data augmentation vision transformer for fine-grained image classification","chao hu, liqiang zhu, weibin qiu, weijie wu","computer vision and pattern recognition (cs.cv)","recently, the vision transformer (vit) has made breakthroughs in image recognition. its self-attention mechanism (msa) can extract discriminative labeling information of different pixel blocks to improve image classification accuracy. however, the classification marks in their deep layers tend to ignore local features between layers. in addition, the embedding layer will be fixed-size pixel blocks. input network inevitably introduces additional image noise. to this end, we study a data augmentation vision transformer (davt) based on data augmentation and proposes a data augmentation method for attention cropping, which uses attention weights as the guide to crop images and improve the ability of the network to learn critical features. secondly, we also propose a hierarchical attention selection (has) method, which improves the ability of discriminative markers between levels of learning by filtering and fusing labels between levels. experimental results show that the accuracy of this method on the two general datasets, cub-200-2011, and stanford dogs, is better than the existing mainstream methods, and its accuracy is 1.4\% and 1.6\% higher than the original vit, respectively","wed, 23 nov 2022 11:34:11 utc (3,347 kb)[v2] thu, 24 nov 2022 08:40:56 utc (3,347 kb)"
"119","detect only what you specify : object detection with linguistic target","moyuru yamada","computer vision and pattern recognition (cs.cv)","object detection is a computer vision task of predicting a set of bounding boxes and category labels for each object of interest in a given image. the category is related to a linguistic symbol such as 'dog' or 'person' and there should be relationships among them. however the object detector only learns to classify the categories and does not treat them as the linguistic symbols. multi-modal models often use the pre-trained object detector to extract object features from the image, but the models are separated from the detector and the extracted visual features does not change with their linguistic input. we rethink the object detection as a vision-and-language reasoning task. we then propose targeted detection task, where detection targets are given by a natural language and the goal of the task is to detect only all the target objects in a given image. there are no detection if the target is not given. commonly used modern object detectors have many hand-designed components like anchor and it is difficult to fuse the textual inputs into the complex pipeline. we thus propose language-targeted detector (ltd) for the targeted detection based on a recently proposed transformer-based detector. ltd is a encoder-decoder architecture and our conditional decoder allows the model to reason about the encoded image with the textual input as the linguistic context. we evaluate detection performances of ltd on coco object detection dataset and also show that our model improves the detection results with the textual input grounding to the visual object.","fri, 18 nov 2022 07:28:47 utc (1,880 kb)"
"120","c3: cross-instance guided contrastive clustering","mohammadreza sadeghi, hadi hojjati, narges armanfard","machine learning (cs.lg)","clustering is the task of gathering similar data samples into clusters without using any predefined labels. it has been widely studied in machine learning literature, and recent advancements in deep learning have revived interest in this field. contrastive clustering (cc) models are a staple of deep clustering in which positive and negative pairs of each data instance are generated through data augmentation. cc models aim to learn a feature space where instance-level and cluster-level representations of positive pairs are grouped together. despite improving the sota, these algorithms ignore the cross-instance patterns, which carry essential information for improving clustering performance. this increases the false-negative-pair rate of the model while decreasing its true-positive-pair rate. in this paper, we propose a novel contrastive clustering method, cross-instance guided contrastive clustering (c3), that considers the cross-sample relationships to increase the number of positive pairs and mitigate the impact of false negative, noise, and anomaly sample on the learned representation of data. in particular, we define a new loss function that identifies similar instances using the instance-level representation and encourages them to aggregate together. moreover, we propose a novel weighting method to select negative samples in a more efficient way. extensive experimental evaluations show that our proposed method can outperform state-of-the-art algorithms on benchmark computer vision datasets: we improve the clustering accuracy by 6.6%, 3.3%, 5.0%, 1.3% and 0.3% on cifar-10, cifar-100, imagenet-10, imagenet-dogs, and tiny-imagenet.","mon, 14 nov 2022 06:28:07 utc (1,123 kb)[v2] mon, 21 nov 2022 22:39:07 utc (1,124 kb)[v3] fri, 21 jul 2023 21:49:20 utc (22,445 kb)[v4] fri, 1 sep 2023 04:27:44 utc (22,445 kb)"
"121","elliptically-contoured tensor-variate distributions with application to improved image learning","carlos llosa-vite, ranjan maitra","methodology (stat.me)","statistical analysis of tensor-valued data has largely used the tensor-variate normal (tvn) distribution that may be inadequate when data comes from distributions with heavier or lighter tails. we study a general family of elliptically contoured (ec) tensor-variate distributions and derive its characterizations, moments, marginal and conditional distributions, and the ec wishart distribution. we describe procedures for maximum likelihood estimation from data that are (1) uncorrelated draws from an ec distribution, (2) from a scale mixture of the tvn distribution, and (3) from an underlying but unknown ec distribution, where we extend tyler's robust estimator. a detailed simulation study highlights the benefits of choosing an ec distribution over the tvn for heavier-tailed data. we develop tensor-variate classification rules using discriminant analysis and ec errors and show that they better predict cats and dogs from images in the animal faces-hq dataset than the tvn-based rules. a novel tensor-on-tensor regression and tensor-variate analysis of variance (tanova) framework under ec errors is also demonstrated to better characterize gender, age and ethnic origin than the usual tvn-based tanova in the celebrated labeled faces of the wild dataset.","sun, 13 nov 2022 16:20:47 utc (5,635 kb)"
"122","spontaneous stable rotation of flocking flexible active matter","gaoxiao jiang, zhihong you, rui ma, chen-xu wu","soft condensed matter (cond-mat.soft)","in nature, active matter, such as worms or dogs, tend to spontaneously form a stable rotational cluster when they flock to the same food source on an unregulated and unconfined surface. {in this paper we present an $n$-node flexible active matter model to study the collective motion due to the flocking of individual agents on a two-dimensional surface, and confirm that there exists a spontaneous stable cluster rotation synchronizing with a chirality produced by the alignment of their bodies under the impetus of the active force.} a prefactor of 1.86 is obtained for the linear relationship between normalized angular velocity and chirality. the angular velocity of such a rotation is found to be dependent on the individual flexibility, the number of nodes in each individual, and the magnitude of the active force. the conclusions well explain the spontaneous stable rotation of clusters that exists in many flexible active matter, like worms or {dogs}, when they flock to the same single source.","thu, 10 nov 2022 04:31:53 utc (7,461 kb)[v2] wed, 16 aug 2023 20:16:01 utc (2,203 kb)"
"123","common pets in 3d: dynamic new-view synthesis of real-life deformable categories","samarth sinha, roman shapovalov, jeremy reizenstein, ignacio rocco, natalia neverova, andrea vedaldi, david novotny","computer vision and pattern recognition (cs.cv)","obtaining photorealistic reconstructions of objects from sparse views is inherently ambiguous and can only be achieved by learning suitable reconstruction priors. earlier works on sparse rigid object reconstruction successfully learned such priors from large datasets such as co3d. in this paper, we extend this approach to dynamic objects. we use cats and dogs as a representative example and introduce common pets in 3d (cop3d), a collection of crowd-sourced videos showing around 4,200 distinct pets. cop3d is one of the first large-scale datasets for benchmarking non-rigid 3d reconstruction ""in the wild"". we also propose tracker-nerf, a method for learning 4d reconstruction from our dataset. at test time, given a small number of video frames of an unseen object, tracker-nerf predicts the trajectories of its 3d points and generates new views, interpolating viewpoint and time. results on cop3d reveal significantly better non-rigid new-view synthesis performance than existing baselines.","mon, 7 nov 2022 22:42:42 utc (9,638 kb)"
"124","contact-free simultaneous sensing of human heart rate and canine breathing rate for animal assisted interactions","timothy holder, mushfiqur rahman, emily summers, david roberts, chau-wai wong, alper bozkurt","signal processing (eess.sp)","animal assisted interventions (aais) involve pleasant interactions between humans and animals and can potentially benefit both types of participants. research in this field may help to uncover universal insights about cross-species bonding, dynamic affect detection, and the influence of environmental factors on dyadic interactions. however, experiments evaluating these outcomes are limited to methodologies that are qualitative, subjective, and cumbersome due to the ergonomic challenges related to attaching sensors to the body. current approaches in aais also face challenges when translating beyond controlled clinical environments or research contexts. these also often neglect the measurements from the animal throughout the interaction. here, we present our preliminary effort toward a contact-free approach to facilitate aai assessment via the physiological sensing of humans and canines using consumer-grade cameras. this initial effort focuses on verifying the technological feasibility of remotely sensing the heart rate signal of the human subject and the breathing rate signal of the dog subject while they are interacting. small amounts of motion such as patting and involuntary body shaking or movement can be tolerated with our custom designed vision-based algorithms. the experimental results show that the physiological measurements obtained by our algorithms were consistent with those provided by the standard reference devices. with further validation and expansion to other physiological parameters, the presented approach offers great promise for many scenarios from the aai research space to veterinary, surgical, and clinical applications.","mon, 7 nov 2022 15:47:31 utc (1,266 kb)[v2] fri, 11 nov 2022 20:03:05 utc (1,266 kb)"
"125","on the informativeness of supervision signals","ilia sucholutsky, ruairidh m. battleday, katherine m. collins, raja marjieh, joshua c. peterson, pulkit singh, umang bhatt, nori jacoby, adrian weller, thomas l. griffiths","machine learning (cs.lg)","supervised learning typically focuses on learning transferable representations from training examples annotated by humans. while rich annotations (like soft labels) carry more information than sparse annotations (like hard labels), they are also more expensive to collect. for example, while hard labels only provide information about the closest class an object belongs to (e.g., ""this is a dog""), soft labels provide information about the object's relationship with multiple classes (e.g., ""this is most likely a dog, but it could also be a wolf or a coyote""). we use information theory to compare how a number of commonly-used supervision signals contribute to representation-learning performance, as well as how their capacity is affected by factors such as the number of labels, classes, dimensions, and noise. our framework provides theoretical justification for using hard labels in the big-data regime, but richer supervision signals for few-shot learning and out-of-distribution generalization. we validate these results empirically in a series of experiments with over 1 million crowdsourced image annotations and conduct a cost-benefit analysis to establish a tradeoff curve that enables users to optimize the cost of supervising representation learning on their own datasets.","wed, 2 nov 2022 18:02:31 utc (2,674 kb)[v2] thu, 22 jun 2023 19:04:25 utc (8,470 kb)[v3] tue, 4 jul 2023 15:25:05 utc (8,470 kb)"
"126","multi-vector retrieval as sparse alignment","yujie qian, jinhyuk lee, sai meher karthik duddu, zhuyun dai, siddhartha brahma, iftekhar naim, tao lei, vincent y. zhao","computation and language (cs.cl)","multi-vector retrieval models improve over single-vector dual encoders on many information retrieval tasks. in this paper, we cast the multi-vector retrieval problem as sparse alignment between query and document tokens. we propose aligner, a novel multi-vector retrieval model that learns sparsified pairwise alignments between query and document tokens (e.g. `dog' vs. `puppy') and per-token unary saliences reflecting their relative importance for retrieval. we show that controlling the sparsity of pairwise token alignments often brings significant performance gains. while most factoid questions focusing on a specific part of a document require a smaller number of alignments, others requiring a broader understanding of a document favor a larger number of alignments. unary saliences, on the other hand, decide whether a token ever needs to be aligned with others for retrieval (e.g. `kind' from `kind of currency is used in new zealand}'). with sparsified unary saliences, we are able to prune a large number of query and document token vectors and improve the efficiency of multi-vector retrieval. we learn the sparse unary saliences with entropy-regularized linear programming, which outperforms other methods to achieve sparsity. in a zero-shot setting, aligner scores 51.1 points ndcg@10, achieving a new retriever-only state-of-the-art on 13 tasks in the beir benchmark. in addition, adapting pairwise alignments with a few examples (<= 8) further improves the performance up to 15.7 points ndcg@10 for argument retrieval tasks. the unary saliences of aligner helps us to keep only 20% of the document token representations with minimal performance loss. we further show that our model often produces interpretable alignments and significantly improves its performance when initialized from larger language models.","wed, 2 nov 2022 16:49:58 utc (696 kb)"
"127","system configuration and navigation of a guide dog robot: toward animal guide dog-level guiding work","hochul hwang, tim xia, ibrahima keita, ken suzuki, joydeep biswas, sunghoon i. lee, donghyun kim","robotics (cs.ro)","a robot guide dog has compelling advantages over animal guide dogs for its cost-effectiveness, potential for mass production, and low maintenance burden. however, despite the long history of guide dog robot research, previous studies were conducted with little or no consideration of how the guide dog handler and the guide dog work as a team for navigation. to develop a robotic guiding system that is genuinely beneficial to blind or visually impaired individuals, we performed qualitative research, including interviews with guide dog handlers and trainers and first-hand blindfold walking experiences with various guide dogs. grounded on the facts learned from vivid experience and interviews, we build a collaborative indoor navigation scheme for a guide dog robot that includes preferred features such as speed and directional control. for collaborative navigation, we propose a semantic-aware local path planner that enables safe and efficient guiding work by utilizing semantic information about the environment and considering the handler's position and directional cues to determine the collision-free path. we evaluate our integrated robotic system by testing guide blindfold walking in indoor settings and demonstrate guide dog-like navigation behavior by avoiding obstacles at typical gait speed ($0.7 \mathrm{m/s}$).","mon, 24 oct 2022 16:11:20 utc (17,442 kb)"
"128","modeling human observer detection in undersampled magnetic resonance imaging (mri) reconstruction with total variation and wavelet sparsity regularization","alexandra g. o'neill (1), emely l. valdez (1), sajan goud lingala (2), angel r. pineda (1) ((1) manhattan college, department of mathematics, the bronx, ny, usa (2) university of iowa, roy j. carver department of biomedical engineering, iowa city, ia, usa)","medical physics (physics.med-ph)","purpose: task-based assessment of image quality in undersampled magnetic resonance imaging provides a way of evaluating the impact of regularization on task performance. in this work, we evaluated the effect of total variation (tv) and wavelet regularization on human detection of signals with a varying background and validated a model observer in predicting human performance. approach: human observer studies used two-alternative forced choice (2-afc) trials with a small signal known exactly task but with varying backgrounds for fluid-attenuated inversion recovery images reconstructed from undersampled multi-coil data. we used a 3.48 undersampling factor with tv and a wavelet sparsity constraints. the sparse difference-of-gaussians (s-dog) observer with internal noise was used to model human observer detection. results: we observed a trend that the human observer detection performance remained fairly constant for a broad range of values in the regularization parameter before decreasing at large values. a similar result was found for the normalized ensemble root mean squared error. without changing the internal noise, the model observer tracked the performance of the human observers as the regularization was increased but overestimated the pc for large amounts of regularization for tv and wavelet sparsity, as well as the combination of both parameters. conclusions: for the task we studied, the s-dog observer was able to reasonably predict human performance with both tv and wavelet sparsity regularizers over a broad range of regularization parameters. we observed a trend that task performance remained fairly constant for a range of regularization parameters before decreasing for large amounts of regularization.","fri, 21 oct 2022 13:41:42 utc (859 kb)[v2] fri, 10 mar 2023 16:01:09 utc (4,004 kb)"
"129","learning attention propagation for compositional zero-shot learning","muhammad gul zain ali khan, muhammad ferjad naeem, luc van gool, alain pagani, didier stricker, muhammad zeshan afzal","computer vision and pattern recognition (cs.cv)","compositional zero-shot learning aims to recognize unseen compositions of seen visual primitives of object classes and their states. while all primitives (states and objects) are observable during training in some combination, their complex interaction makes this task especially hard. for example, wet changes the visual appearance of a dog very differently from a bicycle. furthermore, we argue that relationships between compositions go beyond shared states or objects. a cluttered office can contain a busy table; even though these compositions don't share a state or object, the presence of a busy table can guide the presence of a cluttered office. we propose a novel method called compositional attention propagated embedding (cape) as a solution. the key intuition to our method is that a rich dependency structure exists between compositions arising from complex interactions of primitives in addition to other dependencies between compositions. cape learns to identify this structure and propagates knowledge between them to learn class embedding for all seen and unseen compositions. in the challenging generalized compositional zero-shot setting, we show that our method outperforms previous baselines to set a new state-of-the-art on three publicly available benchmarks.","thu, 20 oct 2022 19:44:11 utc (3,369 kb)"
"130","diffusion models already have a semantic latent space","mingi kwon, jaeseok jeong, youngjung uh","computer vision and pattern recognition (cs.cv)","diffusion models achieve outstanding generative performance in various domains. despite their great success, they lack semantic latent space which is essential for controlling the generative process. to address the problem, we propose asymmetric reverse process (asyrp) which discovers the semantic latent space in frozen pretrained diffusion models. our semantic latent space, named h-space, has nice properties for accommodating semantic image manipulation: homogeneity, linearity, robustness, and consistency across timesteps. in addition, we introduce a principled design of the generative process for versatile editing and quality boost ing by quantifiable measures: editing strength of an interval and quality deficiency at a timestep. our method is applicable to various architectures (ddpm++, id- dpm, and adm) and datasets (celeba-hq, afhq-dog, lsun-church, lsun- bedroom, and metfaces). project page: this https url","thu, 20 oct 2022 02:07:23 utc (14,836 kb)[v2] wed, 29 mar 2023 06:39:50 utc (37,495 kb)"
"131","helpful neighbors: leveraging neighbors in geographic feature pronunciation","llion jones, richard sproat, haruko ishikawa, alexander gutkin","computation and language (cs.cl)","if one sees the place name houston mercer dog run in new york, how does one know how to pronounce it? assuming one knows that houston in new york is pronounced ""how-ston"" and not like the texas city, then one can probably guess that ""how-ston"" is also used in the name of the dog park. we present a novel architecture that learns to use the pronunciations of neighboring names in order to guess the pronunciation of a given target feature. applied to japanese place names, we demonstrate the utility of the model to finding and proposing corrections for errors in google maps. to demonstrate the utility of this approach to structurally similar problems, we also report on an application to a totally different task: cognate reflex prediction in comparative historical linguistics. a version of the code has been open-sourced (this https url).","tue, 18 oct 2022 22:55:16 utc (3,332 kb)"
"132","the tail wagging the dog: dataset construction biases of social bias benchmarks","nikil roashan selvam, sunipa dev, daniel khashabi, tushar khot, kai-wei chang","computation and language (cs.cl)","how reliably can we trust the scores obtained from social bias benchmarks as faithful indicators of problematic social biases in a given language model? in this work, we study this question by contrasting social biases with non-social biases stemming from choices made during dataset construction that might not even be discernible to the human eye. to do so, we empirically simulate various alternative constructions for a given benchmark based on innocuous modifications (such as paraphrasing or random-sampling) that maintain the essence of their social bias. on two well-known social bias benchmarks (winogender and biasnli) we observe that these shallow modifications have a surprising effect on the resulting degree of bias across various models. we hope these troubling observations motivate more robust measures of social biases.","tue, 18 oct 2022 17:58:39 utc (12,248 kb)[v2] fri, 16 jun 2023 18:35:13 utc (6,117 kb)"
"133","imagic: text-based real image editing with diffusion models","bahjat kawar, shiran zada, oran lang, omer tov, huiwen chang, tali dekel, inbar mosseri, michal irani","computer vision and pattern recognition (cs.cv)","text-conditioned image editing has recently attracted considerable interest. however, most methods are currently either limited to specific editing types (e.g., object overlay, style transfer), or apply to synthetically generated images, or require multiple input images of a common object. in this paper we demonstrate, for the very first time, the ability to apply complex (e.g., non-rigid) text-guided semantic edits to a single real image. for example, we can change the posture and composition of one or multiple objects inside an image, while preserving its original characteristics. our method can make a standing dog sit down or jump, cause a bird to spread its wings, etc. -- each within its single high-resolution natural image provided by the user. contrary to previous work, our proposed method requires only a single input image and a target text (the desired edit). it operates on real images, and does not require any additional inputs (such as image masks or additional views of the object). our method, which we call ""imagic"", leverages a pre-trained text-to-image diffusion model for this task. it produces a text embedding that aligns with both the input image and the target text, while fine-tuning the diffusion model to capture the image-specific appearance. we demonstrate the quality and versatility of our method on numerous inputs from various domains, showcasing a plethora of high quality complex semantic image edits, all within a single unified framework.","mon, 17 oct 2022 17:27:32 utc (14,421 kb)[v2] tue, 22 nov 2022 17:34:23 utc (38,610 kb)[v3] mon, 20 mar 2023 15:58:50 utc (27,583 kb)"
"134","can quadruped navigation robots be used as guide dogs?","luyao wang, qihe chen, yan zhang, ziang li, tingmin yan, fan wang, guyue zhou, jiangtao gong","human-computer interaction (cs.hc)","quadruped robots have the potential to guide blind and low vision (blv) people due to their highly flexible locomotion and emotional value provided by their bionic forms. however, the development of quadruped guide robots rarely involves blv users' participatory designs and evaluations. in this paper, we conducted two empirical experiments both in indoor controlled and outdoor field scenarios, exploring the benefits and drawbacks of quadruped guide robots. the results show that the nowadays commercial quadruped robots exposed significant disadvantages in usability and trust compared with wheeled robots. it is concluded that the moving gait and walking noise of quadruped robots would limit the guiding effectiveness to a certain extent, and the empathetic effect of its bionic form for blv users could not be fully reflected. based on the findings of wheeled robots and quadruped robots' advantages, we discuss the design implications for the future guide robot design for blv users. this paper reports the first empirical experiment about quadruped guide robots with blv users and preliminary explores their potential improvement space in substituting guide dogs, which can inspire the further specialized design of quadruped guide robots.","mon, 17 oct 2022 03:44:30 utc (6,921 kb)[v2] tue, 18 oct 2022 07:42:43 utc (6,921 kb)[v3] sat, 24 jun 2023 03:54:36 utc (9,619 kb)"
"135","harfang3d dog-fight sandbox: a reinforcement learning research platform for the customized control tasks of fighter aircrafts","muhammed murat özbek, süleyman yıldırım, muhammet aksoy, eric kernin, emre koyuncu","robotics (cs.ro)","the advent of deep learning (dl) gave rise to significant breakthroughs in reinforcement learning (rl) research. deep reinforcement learning (drl) algorithms have reached super-human level skills when applied to vision-based control problems as such in atari 2600 games where environment states were extracted from pixel information. unfortunately, these environments are far from being applicable to highly dynamic and complex real-world tasks as in autonomous control of a fighter aircraft since these environments only involve 2d representation of a visual world. here, we present a semi-realistic flight simulation environment harfang3d dog-fight sandbox for fighter aircrafts. it is aimed to be a flexible toolbox for the investigation of main challenges in aviation studies using reinforcement learning. the program provides easy access to flight dynamics model, environment states, and aerodynamics of the plane enabling user to customize any specific task in order to build intelligent decision making (control) systems via rl. the software also allows deployment of bot aircrafts and development of multi-agent tasks. this way, multiple groups of aircrafts can be configured to be competitive or cooperative agents to perform complicated tasks including dog fight. during the experiments, we carried out training for two different scenarios: navigating to a designated location and within visual range (wvr) combat, shortly dog fight. using deep reinforcement learning techniques for both scenarios, we were able to train competent agents that exhibit human-like behaviours. based on this results, it is confirmed that harfang3d dog-fight sandbox can be utilized as a 3d realistic rl research platform.","thu, 13 oct 2022 18:18:09 utc (13,737 kb)"
"136","continual learning with evolving class ontologies","zhiqiu lin, deepak pathak, yu-xiong wang, deva ramanan, shu kong","computer vision and pattern recognition (cs.cv)","lifelong learners must recognize concept vocabularies that evolve over time. a common yet underexplored scenario is learning with class labels that continually refine/expand old classes. for example, humans learn to recognize ${\tt dog}$ before dog breeds. in practical settings, dataset $\textit{versioning}$ often introduces refinement to ontologies, such as autonomous vehicle benchmarks that refine a previous ${\tt vehicle}$ class into ${\tt school-bus}$ as autonomous operations expand to new cities. this paper formalizes a protocol for studying the problem of $\textit{learning with evolving class ontology}$ (leco). leco requires learning classifiers in distinct time periods (tps); each tp introduces a new ontology of ""fine"" labels that refines old ontologies of ""coarse"" labels (e.g., dog breeds that refine the previous ${\tt dog}$). leco explores such questions as whether to annotate new data or relabel the old, how to leverage coarse labels, and whether to finetune the previous tp's model or train from scratch. to answer these questions, we leverage insights from related problems such as class-incremental learning. we validate them under the leco protocol through the lens of image classification (cifar and inaturalist) and semantic segmentation (mapillary). our experiments lead to surprising conclusions; while the current status quo is to relabel existing datasets with new ontologies (such as coco-to-lvis or mapillary1.2-to-2.0), leco demonstrates that a far better strategy is to annotate $\textit{new}$ data with the new ontology. however, this produces an aggregate dataset with inconsistent old-vs-new labels, complicating learning. to address this challenge, we adopt methods from semi-supervised and partial-label learning. such strategies can surprisingly be made near-optimal, approaching an ""oracle"" that learns on the aggregate dataset exhaustively labeled with the newest ontology.","mon, 10 oct 2022 19:58:23 utc (3,207 kb)[v2] wed, 12 oct 2022 04:05:37 utc (3,197 kb)[v3] sun, 27 nov 2022 21:58:45 utc (3,197 kb)[v4] thu, 15 dec 2022 02:33:40 utc (3,197 kb)"
"137","an action is worth multiple words: handling ambiguity in action recognition","kiyoon kim, davide moltisanti, oisin mac aodha, laura sevilla-lara","computer vision and pattern recognition (cs.cv)","precisely naming the action depicted in a video can be a challenging and oftentimes ambiguous task. in contrast to object instances represented as nouns (e.g. dog, cat, chair, etc.), in the case of actions, human annotators typically lack a consensus as to what constitutes a specific action (e.g. jogging versus running). in practice, a given video can contain multiple valid positive annotations for the same action. as a result, video datasets often contain significant levels of label noise and overlap between the atomic action classes. in this work, we address the challenge of training multi-label action recognition models from only single positive training labels. we propose two approaches that are based on generating pseudo training examples sampled from similar instances within the train set. unlike other approaches that use model-derived pseudo-labels, our pseudo-labels come from human annotations and are selected based on feature similarity. to validate our approaches, we create a new evaluation benchmark by manually annotating a subset of epic-kitchens-100's validation set with multiple verb labels. we present results on this new test set along with additional results on a new version of hmdb-51, called confusing-hmdb-102, where we outperform existing methods in both cases. data and code are available at this https url","mon, 10 oct 2022 18:06:43 utc (15,350 kb)"
"138","comps: conceptual minimal pair sentences for testing robust property knowledge and its inheritance in pre-trained language models","kanishka misra, julia taylor rayz, allyson ettinger","computation and language (cs.cl)","a characteristic feature of human semantic cognition is its ability to not only store and retrieve the properties of concepts observed through experience, but to also facilitate the inheritance of properties (can breathe) from superordinate concepts (animal) to their subordinates (dog) -- i.e. demonstrate property inheritance. in this paper, we present comps, a collection of minimal pair sentences that jointly tests pre-trained language models (plms) on their ability to attribute properties to concepts and their ability to demonstrate property inheritance behavior. analyses of 22 different plms on comps reveal that they can easily distinguish between concepts on the basis of a property when they are trivially different, but find it relatively difficult when concepts are related on the basis of nuanced knowledge representations. furthermore, we find that plms can demonstrate behavior consistent with property inheritance to a great extent, but fail in the presence of distracting information, which decreases the performance of many models, sometimes even below chance. this lack of robustness in demonstrating simple reasoning raises important questions about plms' capacity to make correct inferences even when they appear to possess the prerequisite knowledge.","wed, 5 oct 2022 00:04:18 utc (1,207 kb)[v2] thu, 6 oct 2022 14:10:29 utc (1,208 kb)[v3] fri, 14 oct 2022 01:57:57 utc (1,208 kb)[v4] thu, 9 feb 2023 02:31:06 utc (835 kb)"
"139","re-imagen: retrieval-augmented text-to-image generator","wenhu chen, hexiang hu, chitwan saharia, william w. cohen","computer vision and pattern recognition (cs.cv)","research on text-to-image generation has witnessed significant progress in generating diverse and photo-realistic images, driven by diffusion and auto-regressive models trained on large-scale image-text data. though state-of-the-art models can generate high-quality images of common entities, they often have difficulty generating images of uncommon entities, such as `chortai (dog)' or `picarones (food)'. to tackle this issue, we present the retrieval-augmented text-to-image generator (re-imagen), a generative model that uses retrieved information to produce high-fidelity and faithful images, even for rare or unseen entities. given a text prompt, re-imagen accesses an external multi-modal knowledge base to retrieve relevant (image, text) pairs and uses them as references to generate the image. with this retrieval step, re-imagen is augmented with the knowledge of high-level semantics and low-level visual details of the mentioned entities, and thus improves its accuracy in generating the entities' visual appearances. we train re-imagen on a constructed dataset containing (image, text, retrieval) triples to teach the model to ground on both text prompt and retrieval. furthermore, we develop a new sampling strategy to interleave the classifier-free guidance for text and retrieval conditions to balance the text and retrieval alignment. re-imagen achieves significant gain on fid score over coco and wikiimage. to further evaluate the capabilities of the model, we introduce entitydrawbench, a new benchmark that evaluates image generation for diverse entities, from frequent to rare, across multiple object categories including dogs, foods, landmarks, birds, and characters. human evaluation on entitydrawbench shows that re-imagen can significantly improve the fidelity of generated images, especially on less frequent entities.","thu, 29 sep 2022 00:57:28 utc (12,075 kb)[v2] sat, 1 oct 2022 15:14:14 utc (16,666 kb)[v3] tue, 22 nov 2022 02:09:38 utc (30,153 kb)"
"140","video-based estimation of pain indicators in dogs","hongyi zhu, yasemin salgırlı, pınar can, durmuş atılgan, albert ali salah","computer vision and pattern recognition (cs.cv)","dog owners are typically capable of recognizing behavioral cues that reveal subjective states of their dogs, such as pain. but automatic recognition of the pain state is very challenging. this paper proposes a novel video-based, two-stream deep neural network approach for this problem. we extract and preprocess body keypoints, and compute features from both keypoints and the rgb representation over the video. we propose an approach to deal with self-occlusions and missing keypoints. we also present a unique video-based dog behavior dataset, collected by veterinary professionals, and annotated for presence of pain, and report good classification results with the proposed approach. this study is one of the first works on machine learning based estimation of dog pain state.","tue, 27 sep 2022 10:38:59 utc (3,596 kb)[v2] sat, 26 nov 2022 15:48:32 utc (3,597 kb)"
"141","hyperdog: an open-source quadruped robot platform based on ros2 and micro-ros","nipun dhananjaya weerakkodi mudalige, iana zhura, ildar babataev, elena nazarova, aleksey fedoseev, dzmitry tsetserukou","robotics (cs.ro)","nowadays, design and development of legged quadruped robots is a quite active area of scientific research. in fact, the legged robots have become popular due to their capabilities to adapt to harsh terrains and diverse environmental conditions in comparison to other mobile robots. with the higher demand for legged robot experiments, more researches and engineers need an affordable and quick way of locomotion algorithm development. in this paper, we present a new open source quadruped robot hyperdog platform, which features 12 rc servo motors, onboard nvidia jetson nano computer and stm32f4 discovery board. hyperdog is an open-source platform for quadruped robotic software development, which is based on robot operating system 2 (ros2) and micro-ros. moreover, the hyperdog is a quadrupedal robotic dog entirely built from 3d printed parts and carbon fiber, which allows the robot to have light weight and good strength. the idea of this work is to demonstrate an affordable and customizable way of robot development and provide researches and engineers with the legged robot platform, where different algorithms can be tested and validated in simulation and real environment. the developed project with code is available on github (this https url).","mon, 19 sep 2022 16:47:18 utc (7,640 kb)"
"142","relationship between incidence of breathing obstruction and degree of muzzle shortness in pedigree dogs","richard d. gill","applications (stat.ap)","there has been much concern about health issues associated with the breeding of short-muzzled pedigree dogs. the dutch government commissioned a scientific report \emph{fokken met kortsnuitige honden} (breeding of short muzzled dogs), van hagen (2019), and based on it rather stringent legislation, restricting breeding primarily on the basis of a single simple measurement of brachycephaly, the cfr: cranial-facial ratio. van hagen's work is a literature study and it draws heavily on statistical results obtained in three publications: njikam (2009), packer et al.~(2015), and liu et al.~(2017). in this paper i discuss some serious shortcomings of those three studies and in particular show that packer et al.\ have drawn unwarranted conclusions from their study. in fact, new analyses using their data leads to an entirely different conclusion.","mon, 19 sep 2022 11:34:06 utc (915 kb)"
"143","improving the environmental perception of autonomous vehicles using deep learning-based audio classification","finley walden, sagar dasgupta, mizanur rahman, mhafuzul islam","sound (cs.sd)","sense of hearing is crucial for autonomous vehicles (avs) to better perceive its surrounding environment. although visual sensors of an av, such as camera, lidar, and radar, help to see its surrounding environment, an av cannot see beyond those sensors line of sight. on the other hand, an av s sense of hearing cannot be obstructed by line of sight. for example, an av can identify an emergency vehicle s siren through audio classification even though the emergency vehicle is not within the line of sight of the av. thus, auditory perception is complementary to the camera, lidar, and radar-based perception systems. this paper presents a deep learning-based robust audio classification framework aiming to achieve improved environmental perception for avs. the presented framework leverages a deep convolution neural network (cnn) to classify different audio classes. urbansound8k, an urban environment dataset, is used to train and test the developed framework. seven audio classes i.e., air conditioner, car horn, children playing, dog bark, engine idling, gunshot, and siren, are identified from the urbansound8k dataset because of their relevancy related to avs. our framework can classify different audio classes with 97.82% accuracy. moreover, the audio classification accuracies with all ten classes are presented, which proves that our framework performed better in the case of av-related sounds compared to the existing audio classification frameworks.","fri, 9 sep 2022 01:23:13 utc (789 kb)"
"144","eating smart: free-ranging dogs follow an optimal foraging strategy while scavenging in groups","rohan sarkar, sreelekshmi r, abhijit nayek, anirban bhowmick, poushali chakraborty, rituparna sonowal, debsruti dasgupta, rounak banerjee, aritra roy, amartya baran mandal, anindita bhadra","populations and evolution (q-bio.pe)","foraging and acquiring of food is a delicate balance between managing the costs, both energy and social, and individual preferences. previous research on the solitary foraging of free ranging dogs showed that they prioritized the nutritionally highest valued food patch first but do not ignore other less valuable food either, displaying typical scavenger behaviour. the current experiment was carried out on groups of dogs with the same set up to see the change in foraging strategies, if any, under the influence of social cost like intra-group competition. we found multiple differences between the strategies of dogs foraging alone versus in groups with competition playing an implicit role in the decision making of dogs when foraging in groups. dogs were able to continually assess and evaluate the available resources in a patch and adjust their behaviour accordingly. foraging in groups also provided benefits of reduced individual vigilance. the various decisions and choices made seemed to have a basis in the optimal foraging theory wherein the dogs harvested the nutritionally richest patch possible with the least risk and cost involved but was willing to compromise if that was not possible. this underscores the cognitive, quick decision-making abilities and adaptable behaviour of these dogs.","thu, 25 aug 2022 06:34:53 utc (672 kb)"
"145","language-guided face animation by recurrent stylegan-based generator","tiankai hang, huan yang, bei liu, jianlong fu, xin geng, baining guo","computer vision and pattern recognition (cs.cv)","recent works on language-guided image manipulation have shown great power of language in providing rich semantics, especially for face images. however, the other natural information, motions, in language is less explored. in this paper, we leverage the motion information and study a novel task, language-guided face animation, that aims to animate a static face image with the help of languages. to better utilize both semantics and motions from languages, we propose a simple yet effective framework. specifically, we propose a recurrent motion generator to extract a series of semantic and motion information from the language and feed it along with visual information to a pre-trained stylegan to generate high-quality frames. to optimize the proposed framework, three carefully designed loss functions are proposed including a regularization loss to keep the face identity, a path length regularization loss to ensure motion smoothness, and a contrastive loss to enable video synthesis with various language guidance in one single model. extensive experiments with both qualitative and quantitative evaluations on diverse domains (\textit{e.g.,} human face, anime face, and dog face) demonstrate the superiority of our model in generating high-quality and realistic videos from one still image with the guidance of language. code will be available at this https url.","thu, 11 aug 2022 02:57:30 utc (30,830 kb)"
"146","scavengers in the human-dominated landscape: an experimental study","sourabh biswas, tathagata bhowmik, kalyan ghosh, anamitra roy, aesha lahiri, sampita sarkar, anindita bhadra","populations and evolution (q-bio.pe)","rapid urbanization is a major cause of habitat and biodiversity loss and human-animal conflict. while urbanization is inevitable, we need to develop a good understanding of the urban ecosystem and the urban-adapted species in order to ensure sustainable cities for our future. scavengers play a major role in urban ecosystems, and often, urban adaptation involves a shift towards scavenging behaviour in wild animals. we carried out an experiment at different sites in the state of west bengal, india, to identify the scavenging guild within urban habitats, in response to human provided food. our study revealed a total of 17 different vertebrate species were identified across sites over 498 sessions of observations. we carried out network analysis to understand the dynamics of the system, and found that the free-ranging dog and common mynah were key species within the scavenging networks. this study revealed the complexity of scavenging networks within human-dominated habitats.","tue, 9 aug 2022 20:22:03 utc (2,766 kb)[v2] thu, 11 aug 2022 09:33:47 utc (2,768 kb)[v3] mon, 17 apr 2023 09:58:14 utc (2,766 kb)"
"147","an overdensity of red galaxies around the hyperluminous dust-obscured quasar w1835$+$4355 at $z=2.3$","yibin luo (ustc), lulu fan (ustc), hu zou (naoc), lu shen (ustc), zesen lin (ustc), weda hu (ustc), zheyu lin (ustc), bojun tao (ustc), guangwen chen (ustc)","astrophysics of galaxies (astro-ph.ga)","\emph{wide-field infrared survey explorer} all-sky survey has discovered a new population of hot dust-obscured galaxies (hot dogs), which has been confirmed to be dusty quasars. previous statistical studies have found significant overdensities of sub-millimeter and mid-ir selected galaxies around hot dogs, indicating they may reside in dense regions. here we present the near-infrared ($j$ and $k_s$ bands) observations over a $7.5'\times 7.5'$ field centered on a hot dog w1835$+$4355 at $z \sim 2.3$ using the wide-field infrared camera on the palomar 200-inch telescope. we use the color criterion $j-k_s>2.3$ for objects with $k_s<20$, to select distant red galaxies (drgs). we find a significant excess of number density of drgs in w1835$+$4355 field compared to three control fields, by a factor of about 2. the overdensity of red galaxies around w1835$+$4355 are consistent with the multi-wavelength environment of hot dogs, suggesting that hot dogs may be a good tracer for dense regions at high redshift. we find that w1835$+$4355 do not reside in the densest region of the dense environment traced by itself. a possible scenario is that w1835$+$4355 is undergoing merging process, which lowers the local number density of galaxies in its surrounding region.","thu, 28 jul 2022 18:00:02 utc (5,370 kb)"
"148","contrastive monotonic pixel-level modulation","kun lu, rongpeng li, honggang zhang","computer vision and pattern recognition (cs.cv)","continuous one-to-many mapping is a less investigated yet important task in both low-level visions and neural image translation. in this paper, we present a new formulation called monopix, an unsupervised and contrastive continuous modulation model, and take a step further to enable a pixel-level spatial control which is critical but can not be properly handled previously. the key feature of this work is to model the monotonicity between controlling signals and the domain discriminator with a novel contrastive modulation framework and corresponding monotonicity constraints. we have also introduced a selective inference strategy with logarithmic approximation complexity and support fast domain adaptations. the state-of-the-art performance is validated on a variety of continuous mapping tasks, including afhq cat-dog and yosemite summer-winter translation. the introduced approach also helps to provide a new solution for many low-level tasks like low-light enhancement and natural noise generation, which is beyond the long-established practice of one-to-one training and inference. code is available at this https url.","sat, 23 jul 2022 13:21:24 utc (21,823 kb)"
"149","adaptive soft contrastive learning","chen feng, ioannis patras","computer vision and pattern recognition (cs.cv)","self-supervised learning has recently achieved great success in representation learning without human annotations. the dominant method -- that is contrastive learning, is generally based on instance discrimination tasks, i.e., individual samples are treated as independent categories. however, presuming all the samples are different contradicts the natural grouping of similar samples in common visual datasets, e.g., multiple views of the same dog. to bridge the gap, this paper proposes an adaptive method that introduces soft inter-sample relations, namely adaptive soft contrastive learning (ascl). more specifically, ascl transforms the original instance discrimination task into a multi-instance soft discrimination task, and adaptively introduces inter-sample relations. as an effective and concise plug-in module for existing self-supervised learning frameworks, ascl achieves the best performance on several benchmarks in terms of both performance and efficiency. code is available at this https url.","fri, 22 jul 2022 16:01:07 utc (4,745 kb)"
"150","benchmarking omni-vision representation through the lens of visual realms","yuanhan zhang, zhenfei yin, jing shao, ziwei liu","computer vision and pattern recognition (cs.cv)","though impressive performance has been achieved in specific visual realms (e.g. faces, dogs, and places), an omni-vision representation generalizing to many natural visual domains is highly desirable. but, existing benchmarks are biased and inefficient to evaluate the omni-vision representation -- these benchmarks either only include several specific realms, or cover most realms at the expense of subsuming numerous datasets that have extensive realm overlapping. in this paper, we propose omni-realm benchmark (omnibenchmark). it includes 21 realm-wise datasets with 7,372 concepts and 1,074,346 images. without semantic overlapping, these datasets cover most visual realms comprehensively and meanwhile efficiently. in addition, we propose a new supervised contrastive learning framework, namely relational contrastive learning (reco), for a better omni-vision representation. beyond pulling two instances from the same concept closer -- the typical supervised contrastive learning framework -- reco also pulls two instances from the same semantic realm closer, encoding the semantic relation between concepts, and facilitating omni-vision representation learning. we benchmark reco and other advances in omni-vision representation studies that are different in architectures (from cnns to transformers) and in learning paradigms (from supervised learning to self-supervised learning) on omnibenchmark. we illustrate the superior of reco to other supervised contrastive learning methods and reveal multiple practical observations to facilitate future research.","thu, 14 jul 2022 17:58:02 utc (6,561 kb)[v2] fri, 15 jul 2022 03:34:49 utc (6,561 kb)"
